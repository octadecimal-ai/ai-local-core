# Migracja do FastAPI i rozw√≥j serwis√≥w AI

**Data utworzenia:** 2025-11-13 18:12:26  
**Dla:** Cursor AI Assistant pracujƒÖcego w projekcie `ai-local-core`  
**Cel:** Kompleksowa instrukcja migracji z Flask do FastAPI oraz rozwoju nowych serwis√≥w (ai-joker, ai-joke-analyser)

---

## üìã SPIS TRE≈öCI

1. [Przygotowanie ≈õrodowiska](#1-przygotowanie-≈õrodowiska)
2. [Migracja z Flask do FastAPI](#2-migracja-z-flask-do-fastapi)
3. [Instalacja komponent√≥w](#3-instalacja-komponent√≥w)
4. [Architektura modularna](#4-architektura-modularna)
5. [Rozw√≥j serwisu ai-joker](#5-rozw√≥j-serwisu-ai-joker)
6. [Rozw√≥j serwisu ai-joke-analyser](#6-rozw√≥j-serwisu-ai-joke-analyser)
7. [Konfiguracja ≈õrodowiskowa](#7-konfiguracja-≈õrodowiskowa)
8. [Testowanie](#8-testowanie)
9. [Deployment](#9-deployment)

---

## 1. PRZYGOTOWANIE ≈öRODOWISKA

### 1.1. Wymagania wstƒôpne

- Python 3.10+ (sprawd≈∫: `python3 --version`)
- Virtual environment (venv) - ju≈º istnieje w projekcie
- Dostƒôp do GPU (opcjonalnie, dla Bielik 7B)
- Dostƒôp do Ollama (dla istniejƒÖcych serwis√≥w)

### 1.2. Aktywacja ≈õrodowiska wirtualnego

```bash
cd /Users/piotradamczyk/Projects/Octadecimal/ai-local-core
source venv/bin/activate  # Linux/Mac
# lub
venv\Scripts\activate  # Windows
```

### 1.3. Backup istniejƒÖcego kodu - niepotrzebne - pomijamy

**WA≈ªNE:** Przed rozpoczƒôciem migracji wykonaj backup:

```bash
# Utw√≥rz branch dla migracji
git checkout -b feature/migracja-fastapi

# Lub skopiuj istniejƒÖcy server.py
cp src/api/server.py src/api/server.py.flask-backup
```

---

## 2. MIGRACJA Z FLASK DO FASTAPI

### 2.1. Aktualizacja requirements.txt

Dodaj FastAPI i zale≈ºno≈õci do `requirements.txt`:

```txt
# IstniejƒÖce zale≈ºno≈õci
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
pillow>=9.5.0
requests>=2.31.0
deep-translator>=1.11.4
ollama>=0.3.0

# FastAPI i zale≈ºno≈õci
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
pydantic-settings>=2.1.0
python-multipart>=0.0.6

# Opcjonalnie (dla lepszej dokumentacji)
python-jose[cryptography]>=3.3.0
```

**Instalacja:**

```bash
pip install -r requirements.txt
```

### 2.2. Struktura katalog√≥w (nowa)

Utw√≥rz modularnƒÖ strukturƒô:

```
src/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # G≈Ç√≥wny plik FastAPI (nowy)
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Konfiguracja (nowy)
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py       # Zale≈ºno≈õci (nowy)
‚îÇ   ‚îî‚îÄ‚îÄ server.py             # Flask (stary, do usuniƒôcia po migracji)
‚îÇ
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ image/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py         # Router FastAPI dla obrazk√≥w
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py         # Logika biznesowa
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ollama/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py         # Router FastAPI dla Ollama
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py         # Logika biznesowa
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ joker/                # NOWY MODU≈Å
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py          # Modele Pydantic
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ joke_analyser/        # NOWY MODU≈Å
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ router.py
‚îÇ       ‚îú‚îÄ‚îÄ service.py
‚îÇ       ‚îî‚îÄ‚îÄ models.py
‚îÇ
‚îú‚îÄ‚îÄ image/                    # IstniejƒÖce (do refaktoryzacji)
‚îú‚îÄ‚îÄ ollama/                   # IstniejƒÖce (do refaktoryzacji)
‚îú‚îÄ‚îÄ translation/              # IstniejƒÖce
‚îî‚îÄ‚îÄ polling/                  # IstniejƒÖce
```

**Utworzenie struktury:**

```bash
mkdir -p src/modules/{image,ollama,joker,joke_analyser}
touch src/modules/__init__.py
touch src/modules/image/{__init__.py,router.py,service.py}
touch src/modules/ollama/{__init__.py,router.py,service.py}
touch src/modules/joker/{__init__.py,router.py,service.py,models.py}
touch src/modules/joke_analyser/{__init__.py,router.py,service.py,models.py}
touch src/api/{main.py,config.py,dependencies.py}
```

### 2.3. Konfiguracja (config.py)

Utw√≥rz `src/api/config.py`:

```python
"""
Konfiguracja aplikacji FastAPI
Wspiera modularnƒÖ architekturƒô z mo≈ºliwo≈õciƒÖ w≈ÇƒÖczania/wy≈ÇƒÖczania modu≈Ç√≥w
"""

from pydantic_settings import BaseSettings
from typing import Optional


class ServiceConfig(BaseSettings):
    """Konfiguracja serwisu"""
    
    # Podstawowe ustawienia
    SERVICE_NAME: str = "ai-local-core"
    HOST: str = "127.0.0.1"
    PORT: int = 5001
    DEBUG: bool = False
    
    # W≈ÇƒÖczanie/wy≈ÇƒÖczanie modu≈Ç√≥w
    ENABLE_IMAGE_DESCRIPTION: bool = True
    ENABLE_OLLAMA: bool = True
    ENABLE_JOKER: bool = False
    ENABLE_JOKE_ANALYSER: bool = False
    
    # Konfiguracja modu≈Ç√≥w
    # Image Description
    IMAGE_MODEL_NAME: str = "Salesforce/blip-image-captioning-base"
    
    # Ollama
    OLLAMA_BASE_URL: str = "http://localhost:11434"
    OLLAMA_DEFAULT_MODEL: str = "llama2"
    
    # Joker (Bielik 7B)
    JOKER_MODEL_PATH: Optional[str] = None  # ≈öcie≈ºka do modelu lokalnego
    JOKER_MODEL_NAME: str = "bielik-7b-v0.1"
    JOKER_USE_GPU: bool = True
    JOKER_QUANTIZATION: str = "int8"  # int4, int8, fp16
    
    # Joke Analyser
    JOKE_ANALYSER_MODEL_NAME: str = "allegro/herbert-base-cased"
    JOKE_ANALYSER_USE_GPU: bool = False  # CPU wystarczy
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False


# Globalna instancja konfiguracji
config = ServiceConfig()
```

### 2.4. G≈Ç√≥wny plik FastAPI (main.py)

Utw√≥rz `src/api/main.py`:

```python
#!/usr/bin/env python3
"""
FastAPI server dla ai-local-core
Modularna architektura z mo≈ºliwo≈õciƒÖ w≈ÇƒÖczania/wy≈ÇƒÖczania modu≈Ç√≥w
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging
import sys
import os

# Dodaj ≈õcie≈ºkƒô do src do PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from api.config import config
from api.dependencies import get_logger

# Setup logging
logging.basicConfig(
    level=logging.INFO if not config.DEBUG else logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = get_logger(__name__)

# Inicjalizacja FastAPI
app = FastAPI(
    title=config.SERVICE_NAME,
    description="Lokalne serwisy AI dla Waldus API",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # W produkcji ograniczyƒá do konkretnych domen
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Health check
@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": config.SERVICE_NAME,
        "version": "2.0.0"
    }


# Warunkowe w≈ÇƒÖczanie modu≈Ç√≥w
if config.ENABLE_IMAGE_DESCRIPTION:
    try:
        from modules.image.router import router as image_router
        app.include_router(image_router, prefix="/describe", tags=["Image"])
        logger.info("‚úÖ Modu≈Ç Image Description w≈ÇƒÖczony")
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd w≈ÇƒÖczania modu≈Çu Image Description: {e}")

if config.ENABLE_OLLAMA:
    try:
        from modules.ollama.router import router as ollama_router
        app.include_router(ollama_router, prefix="/ollama", tags=["Ollama"])
        logger.info("‚úÖ Modu≈Ç Ollama w≈ÇƒÖczony")
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd w≈ÇƒÖczania modu≈Çu Ollama: {e}")

if config.ENABLE_JOKER:
    try:
        from modules.joker.router import router as joker_router
        app.include_router(joker_router, prefix="/joker", tags=["Joker"])
        logger.info("‚úÖ Modu≈Ç Joker w≈ÇƒÖczony")
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd w≈ÇƒÖczania modu≈Çu Joker: {e}")

if config.ENABLE_JOKE_ANALYSER:
    try:
        from modules.joke_analyser.router import router as analyser_router
        app.include_router(analyser_router, prefix="/joke-analyser", tags=["Joke Analyser"])
        logger.info("‚úÖ Modu≈Ç Joke Analyser w≈ÇƒÖczony")
    except Exception as e:
        logger.error(f"‚ùå B≈ÇƒÖd w≈ÇƒÖczania modu≈Çu Joke Analyser: {e}")


# Globalny handler b≈Çƒôd√≥w
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Nieoczekiwany b≈ÇƒÖd: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "Wewnƒôtrzny b≈ÇƒÖd serwera",
            "detail": str(exc) if config.DEBUG else None
        }
    )


if __name__ == "__main__":
    import uvicorn
    logger.info(f"üöÄ Uruchamianie {config.SERVICE_NAME} na {config.HOST}:{config.PORT}")
    uvicorn.run(
        "main:app",
        host=config.HOST,
        port=config.PORT,
        reload=config.DEBUG,
        log_level="info"
    )
```

### 2.5. Zale≈ºno≈õci (dependencies.py)

Utw√≥rz `src/api/dependencies.py`:

```python
"""
Zale≈ºno≈õci FastAPI (dependency injection)
"""

import logging
from functools import lru_cache


@lru_cache()
def get_logger(name: str) -> logging.Logger:
    """Zwraca logger dla danego modu≈Çu"""
    return logging.getLogger(name)
```

### 2.6. Migracja istniejƒÖcych endpoint√≥w

#### 2.6.1. Image Description Router

Utw√≥rz `src/modules/image/router.py`:

```python
"""
Router FastAPI dla opisu obrazk√≥w
Migracja z Flask: /describe
"""

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, HttpUrl
from typing import Optional
import logging

from api.config import config
from api.dependencies import get_logger
from image.describe import describe_image, load_model

logger = get_logger(__name__)
router = APIRouter()

# Za≈Çaduj model przy starcie
logger.info("Inicjalizacja modu≈Çu Image Description...")
try:
    load_model()
    logger.info("‚úÖ Modu≈Ç Image Description gotowy")
except Exception as e:
    logger.error(f"‚ùå B≈ÇƒÖd inicjalizacji modu≈Çu Image Description: {e}")


class ImageDescriptionRequest(BaseModel):
    """Request model dla opisu obrazka"""
    image_url: HttpUrl
    max_length: Optional[int] = 50


class ImageDescriptionResponse(BaseModel):
    """Response model dla opisu obrazka"""
    success: bool
    description: Optional[str] = None
    image_url: str
    error: Optional[str] = None


@router.post("/", response_model=ImageDescriptionResponse)
async def describe(
    request: ImageDescriptionRequest,
    logger: logging.Logger = Depends(get_logger)
):
    """
    Opisz obrazek
    
    - **image_url**: URL obrazka do opisu
    - **max_length**: Maksymalna d≈Çugo≈õƒá opisu (domy≈õlnie 50)
    """
    try:
        logger.info(f"Opisywanie obrazka: {request.image_url}")
        description = describe_image(str(request.image_url), request.max_length)
        
        return ImageDescriptionResponse(
            success=True,
            description=description,
            image_url=str(request.image_url)
        )
    except Exception as e:
        logger.error(f"B≈ÇƒÖd opisywania obrazka: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"B≈ÇƒÖd opisywania obrazka: {str(e)}"
        )
```

#### 2.6.2. Ollama Router

Utw√≥rz `src/modules/ollama/router.py`:

```python
"""
Router FastAPI dla Ollama
Migracja z Flask: /ollama/chat
"""

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import Optional
import logging

from api.config import config
from api.dependencies import get_logger
from ollama.client import OllamaClient

logger = get_logger(__name__)
router = APIRouter()

# Inicjalizacja klienta Ollama
ollama_client = OllamaClient()


class OllamaChatRequest(BaseModel):
    """Request model dla chat Ollama"""
    user: str
    system: Optional[str] = None
    task: Optional[str] = None
    model: Optional[str] = None
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 1000


class OllamaChatResponse(BaseModel):
    """Response model dla chat Ollama"""
    success: bool
    response: Optional[str] = None
    usage: Optional[dict] = None
    model: Optional[str] = None
    error: Optional[str] = None


@router.post("/chat", response_model=OllamaChatResponse)
async def chat(
    request: OllamaChatRequest,
    logger: logging.Logger = Depends(get_logger)
):
    """
    Wykonaj zapytanie do Ollama z przekazanym promptem
    
    - **user**: Wiadomo≈õƒá u≈ºytkownika (wymagane)
    - **system**: Opcjonalny system prompt
    - **task**: Opcjonalne dodatkowe instrukcje
    - **model**: Opcjonalna nazwa modelu
    - **temperature**: Temperatura (domy≈õlnie 0.7)
    - **max_tokens**: Maksymalna liczba token√≥w (domy≈õlnie 1000)
    """
    try:
        user_message = request.user
        if request.task:
            user_message = f"{user_message}\n\nZADANIE:\n{request.task}"
        
        logger.info(f"Wysy≈Çanie zapytania do Ollama (model={request.model or ollama_client.default_model})")
        result = ollama_client.chat(
            user=user_message,
            system=request.system,
            model=request.model,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        return OllamaChatResponse(
            success=True,
            response=result['text'],
            usage=result['usage'],
            model=result['raw'].get('model', request.model or ollama_client.default_model)
        )
    except ValueError as e:
        logger.error(f"B≈ÇƒÖd walidacji zapytania do Ollama: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas komunikacji z Ollama: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## 3. INSTALACJA KOMPONENT√ìW

### 3.1. Zale≈ºno≈õci Python

```bash
# Zainstaluj wszystkie zale≈ºno≈õci
pip install -r requirements.txt

# Sprawd≈∫ instalacjƒô FastAPI
python -c "import fastapi; print(fastapi.__version__)"
```

### 3.2. Bielik 7B (dla ai-joker)

**Opcja A: MLX (Mac M1/M5) - REKOMENDOWANE**

```bash
# Instalacja MLX
pip install mlx mlx-lm

# Pobranie modelu Bielik 7B
python -c "
from mlx_lm import load, generate
model, tokenizer = load('piotradamczyk/bielik-7b-v0.1')
"
```

**Opcja B: llama.cpp (Mac/Ubuntu)**

```bash
# Instalacja llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make

# Pobranie modelu (GGUF format)
# Instrukcje: https://huggingface.co/piotradamczyk/bielik-7b-v0.1
```

**Opcja C: Transformers (Ubuntu z GPU)**

```bash
# Zainstaluj transformers (ju≈º jest w requirements.txt)
# Model zostanie pobrany automatycznie przy pierwszym u≈ºyciu
```

### 3.3. HerBERT (dla ai-joke-analyser)

```bash
# HerBERT jest ju≈º dostƒôpny przez transformers
# Pobierze siƒô automatycznie przy pierwszym u≈ºyciu

# Sprawd≈∫ instalacjƒô
python -c "
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('allegro/herbert-base-cased')
print('‚úÖ HerBERT zainstalowany')
"
```

### 3.4. spaCy (dla ai-joke-analyser)

```bash
# Instalacja spaCy
pip install spacy

# Pobranie modelu polskiego
python -m spacy download pl_core_news_sm

# Sprawd≈∫ instalacjƒô
python -c "import spacy; nlp = spacy.load('pl_core_news_sm'); print('‚úÖ spaCy PL zainstalowany')"
```

---

## 4. ARCHITEKTURA MODULARNA

### 4.1. Koncepcja

FastAPI pozwala na **modularnƒÖ architekturƒô**, gdzie ka≈ºdy serwis jest osobnym routerem:

```
FastAPI App (main.py)
‚îú‚îÄ‚îÄ /describe (Image Description) - warunkowo w≈ÇƒÖczony
‚îú‚îÄ‚îÄ /ollama (Ollama) - warunkowo w≈ÇƒÖczony
‚îú‚îÄ‚îÄ /joker (Joker) - warunkowo w≈ÇƒÖczony
‚îî‚îÄ‚îÄ /joke-analyser (Joke Analyser) - warunkowo w≈ÇƒÖczony
```

### 4.2. Wzorzec routera

Ka≈ºdy modu≈Ç powinien mieƒá:

1. **router.py** - Endpointy FastAPI
2. **service.py** - Logika biznesowa
3. **models.py** - Modele Pydantic (request/response)

**Przyk≈Çad struktury:**

```python
# modules/joker/router.py
from fastapi import APIRouter, Depends
from .service import JokerService
from .models import JokeRequest, JokeResponse

router = APIRouter()
service = JokerService()

@router.post("/generate", response_model=JokeResponse)
async def generate_joke(request: JokeRequest):
    return await service.generate(request)
```

---

## 5. ROZW√ìJ SERWISU AI-JOKER

### 5.1. Modele Pydantic

Utw√≥rz `src/modules/joker/models.py`:

```python
"""
Modele Pydantic dla serwisu Joker
"""

from pydantic import BaseModel, Field
from typing import Optional, List


class JokeRequest(BaseModel):
    """Request model dla generowania ≈ºartu"""
    topic: Optional[str] = Field(None, description="Temat ≈ºartu")
    style: Optional[str] = Field("sarcastic", description="Styl ≈ºartu (sarcastic, witty, absurd)")
    length: Optional[str] = Field("medium", description="D≈Çugo≈õƒá (short, medium, long)")
    temperature: Optional[float] = Field(0.8, ge=0.0, le=2.0, description="Temperatura generowania")
    max_tokens: Optional[int] = Field(200, ge=50, le=500, description="Maksymalna liczba token√≥w")


class JokeResponse(BaseModel):
    """Response model dla wygenerowanego ≈ºartu"""
    success: bool
    joke: Optional[str] = None
    topic: Optional[str] = None
    style: Optional[str] = None
    generation_time: Optional[float] = None
    model: Optional[str] = None
    error: Optional[str] = None
```

### 5.2. Serwis (service.py)

Utw√≥rz `src/modules/joker/service.py`:

```python
"""
Serwis generowania ≈ºart√≥w u≈ºywajƒÖcy Bielik 7B
"""

import time
import logging
from typing import Optional
from .models import JokeRequest, JokeResponse
from api.config import config

logger = logging.getLogger(__name__)


class JokerService:
    """Serwis generowania ≈ºart√≥w"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Za≈Çaduj model Bielik 7B"""
        try:
            logger.info(f"≈Åadowanie modelu Bielik: {config.JOKER_MODEL_NAME}")
            
            # Opcja A: MLX (Mac)
            if config.JOKER_USE_GPU and hasattr(config, 'USE_MLX'):
                from mlx_lm import load
                self.model, self.tokenizer = load(config.JOKER_MODEL_NAME)
                logger.info("‚úÖ Model za≈Çadowany przez MLX")
            
            # Opcja B: Transformers (Ubuntu z GPU)
            else:
                from transformers import AutoTokenizer, AutoModelForCausalLM
                import torch
                
                device = "cuda" if config.JOKER_USE_GPU and torch.cuda.is_available() else "cpu"
                self.tokenizer = AutoTokenizer.from_pretrained(config.JOKER_MODEL_NAME)
                self.model = AutoModelForCausalLM.from_pretrained(
                    config.JOKER_MODEL_NAME,
                    torch_dtype=torch.float16 if config.JOKER_QUANTIZATION == "fp16" else torch.float32,
                    device_map="auto" if device == "cuda" else None
                )
                if device == "cpu":
                    self.model = self.model.to(device)
                logger.info(f"‚úÖ Model za≈Çadowany przez Transformers na {device}")
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd ≈Çadowania modelu: {e}")
            raise
    
    async def generate(self, request: JokeRequest) -> JokeResponse:
        """
        Wygeneruj ≈ºart na podstawie requestu
        """
        start_time = time.time()
        
        try:
            # Przygotuj prompt
            prompt = self._build_prompt(request)
            
            # Generuj ≈ºart
            joke = await self._generate_text(prompt, request)
            
            generation_time = time.time() - start_time
            
            return JokeResponse(
                success=True,
                joke=joke,
                topic=request.topic,
                style=request.style,
                generation_time=generation_time,
                model=config.JOKER_MODEL_NAME
            )
        
        except Exception as e:
            logger.error(f"B≈ÇƒÖd generowania ≈ºartu: {e}")
            return JokeResponse(
                success=False,
                error=str(e)
            )
    
    def _build_prompt(self, request: JokeRequest) -> str:
        """Zbuduj prompt dla modelu"""
        prompt_parts = []
        
        if request.topic:
            prompt_parts.append(f"Temat: {request.topic}")
        
        prompt_parts.append(f"Styl: {request.style}")
        prompt_parts.append(f"D≈Çugo≈õƒá: {request.length}")
        prompt_parts.append("\nWygeneruj ≈ºart:")
        
        return "\n".join(prompt_parts)
    
    async def _generate_text(self, prompt: str, request: JokeRequest) -> str:
        """Wygeneruj tekst u≈ºywajƒÖc modelu"""
        # Implementacja zale≈ºna od wybranej biblioteki
        # Przyk≈Çad dla MLX:
        if hasattr(self, 'model') and hasattr(self.model, 'generate'):
            from mlx_lm import generate
            response = generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=request.max_tokens,
                temp=request.temperature
            )
            return response
        
        # Przyk≈Çad dla Transformers:
        else:
            inputs = self.tokenizer(prompt, return_tensors="pt")
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                do_sample=True
            )
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### 5.3. Router

Utw√≥rz `src/modules/joker/router.py`:

```python
"""
Router FastAPI dla serwisu Joker
"""

from fastapi import APIRouter, HTTPException, Depends
import logging
from .service import JokerService
from .models import JokeRequest, JokeResponse
from api.config import config
from api.dependencies import get_logger

logger = get_logger(__name__)
router = APIRouter()

# Inicjalizacja serwisu
try:
    service = JokerService()
    logger.info("‚úÖ Serwis Joker zainicjalizowany")
except Exception as e:
    logger.error(f"‚ùå B≈ÇƒÖd inicjalizacji serwisu Joker: {e}")
    service = None


@router.post("/generate", response_model=JokeResponse)
async def generate_joke(
    request: JokeRequest,
    logger: logging.Logger = Depends(get_logger)
):
    """
    Wygeneruj ≈ºart u≈ºywajƒÖc Bielik 7B
    
    - **topic**: Temat ≈ºartu (opcjonalne)
    - **style**: Styl ≈ºartu (sarcastic, witty, absurd) - domy≈õlnie sarcastic
    - **length**: D≈Çugo≈õƒá (short, medium, long) - domy≈õlnie medium
    - **temperature**: Temperatura generowania (0.0-2.0) - domy≈õlnie 0.8
    - **max_tokens**: Maksymalna liczba token√≥w (50-500) - domy≈õlnie 200
    """
    if service is None:
        raise HTTPException(
            status_code=503,
            detail="Serwis Joker nie jest dostƒôpny"
        )
    
    logger.info(f"Generowanie ≈ºartu: topic={request.topic}, style={request.style}")
    return await service.generate(request)


@router.get("/health")
async def health():
    """Health check dla serwisu Joker"""
    return {
        "status": "healthy" if service is not None else "unavailable",
        "model": config.JOKER_MODEL_NAME if service else None
    }
```

---

## 6. ROZW√ìJ SERWISU AI-JOKE-ANALYSER

### 6.1. Modele Pydantic

Utw√≥rz `src/modules/joke_analyser/models.py`:

```python
"""
Modele Pydantic dla serwisu Joke Analyser
"""

from pydantic import BaseModel, Field
from typing import Optional, List, Dict


class JokeAnalysisRequest(BaseModel):
    """Request model dla analizy ≈ºartu"""
    joke: str = Field(..., description="Tekst ≈ºartu do analizy")
    techniques: Optional[List[str]] = Field(
        None,
        description="Lista technik analizy (opcjonalne, domy≈õlnie wszystkie)"
    )


class TechniqueScore(BaseModel):
    """Wynik dla jednej techniki analizy"""
    technique: str
    score: float
    explanation: str


class JokeAnalysisResponse(BaseModel):
    """Response model dla analizy ≈ºartu"""
    success: bool
    joke: str
    overall_score: Optional[float] = None
    techniques: Optional[List[TechniqueScore]] = None
    sentiment: Optional[str] = None
    keywords: Optional[List[str]] = None
    analysis_time: Optional[float] = None
    error: Optional[str] = None
```

### 6.2. Serwis (service.py)

Utw√≥rz `src/modules/joke_analyser/service.py`:

```python
"""
Serwis analizy ≈ºart√≥w u≈ºywajƒÖcy HerBERT + spaCy
"""

import time
import logging
from typing import List, Dict, Optional
import spacy
from transformers import AutoTokenizer, AutoModel
import torch

from .models import JokeAnalysisRequest, JokeAnalysisResponse, TechniqueScore
from api.config import config

logger = logging.getLogger(__name__)


class JokeAnalyserService:
    """Serwis analizy ≈ºart√≥w"""
    
    # Lista dostƒôpnych technik (z pliku techniki-rozk≈Çadu-≈ºartu-na-czynniki-pierwsze.txt)
    AVAILABLE_TECHNIQUES = [
        "incongruity",
        "archetypes",
        "psychoanalysis",
        "setup_punchline",
        "semantic_shift",
        "absurd_escalation",
        "timing",
        "humor_micro_components",
        "reverse_engineering"
    ]
    
    def __init__(self):
        self.herbert_tokenizer = None
        self.herbert_model = None
        self.spacy_nlp = None
        self._load_models()
    
    def _load_models(self):
        """Za≈Çaduj modele HerBERT i spaCy"""
        try:
            logger.info("≈Åadowanie modeli analizy ≈ºart√≥w...")
            
            # HerBERT
            logger.info("≈Åadowanie HerBERT...")
            self.herbert_tokenizer = AutoTokenizer.from_pretrained(config.JOKE_ANALYSER_MODEL_NAME)
            self.herbert_model = AutoModel.from_pretrained(config.JOKE_ANALYSER_MODEL_NAME)
            
            device = "cuda" if config.JOKE_ANALYSER_USE_GPU and torch.cuda.is_available() else "cpu"
            self.herbert_model = self.herbert_model.to(device)
            self.herbert_model.eval()
            logger.info(f"‚úÖ HerBERT za≈Çadowany na {device}")
            
            # spaCy
            logger.info("≈Åadowanie spaCy...")
            self.spacy_nlp = spacy.load("pl_core_news_sm")
            logger.info("‚úÖ spaCy za≈Çadowany")
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd ≈Çadowania modeli: {e}")
            raise
    
    async def analyze(self, request: JokeAnalysisRequest) -> JokeAnalysisResponse:
        """
        Przeanalizuj ≈ºart u≈ºywajƒÖc r√≥≈ºnych technik
        """
        start_time = time.time()
        
        try:
            # Wybierz techniki do analizy
            techniques_to_analyze = request.techniques or self.AVAILABLE_TECHNIQUES
            
            # Wykonaj analizƒô dla ka≈ºdej techniki
            technique_scores = []
            for technique in techniques_to_analyze:
                score = await self._analyze_technique(request.joke, technique)
                technique_scores.append(score)
            
            # Oblicz og√≥lny wynik
            overall_score = sum(t.score for t in technique_scores) / len(technique_scores) if technique_scores else 0.0
            
            # Analiza sentymentu i s≈Ç√≥w kluczowych
            sentiment = await self._analyze_sentiment(request.joke)
            keywords = await self._extract_keywords(request.joke)
            
            analysis_time = time.time() - start_time
            
            return JokeAnalysisResponse(
                success=True,
                joke=request.joke,
                overall_score=overall_score,
                techniques=technique_scores,
                sentiment=sentiment,
                keywords=keywords,
                analysis_time=analysis_time
            )
        
        except Exception as e:
            logger.error(f"B≈ÇƒÖd analizy ≈ºartu: {e}")
            return JokeAnalysisResponse(
                success=False,
                joke=request.joke,
                error=str(e)
            )
    
    async def _analyze_technique(self, joke: str, technique: str) -> TechniqueScore:
        """Przeanalizuj ≈ºart pod kƒÖtem konkretnej techniki"""
        # Implementacja zale≈ºna od techniki
        # Przyk≈Çad uproszczony:
        
        # U≈ºyj HerBERT do ekstrakcji cech
        inputs = self.herbert_tokenizer(joke, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.herbert_model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        # Uproszczony scoring (w rzeczywisto≈õci potrzebny trenowany klasyfikator)
        score = float(embeddings.mean().item()) % 1.0  # Przyk≈Çad
        
        explanation = f"Analiza techniki {technique} dla ≈ºartu: {joke[:50]}..."
        
        return TechniqueScore(
            technique=technique,
            score=score,
            explanation=explanation
        )
    
    async def _analyze_sentiment(self, joke: str) -> str:
        """Analiza sentymentu ≈ºartu"""
        # Uproszczona analiza (w rzeczywisto≈õci potrzebny model sentymentu)
        doc = self.spacy_nlp(joke)
        # Implementacja analizy sentymentu
        return "positive"  # Placeholder
    
    async def _extract_keywords(self, joke: str) -> List[str]:
        """WyciƒÖgnij s≈Çowa kluczowe z ≈ºartu"""
        doc = self.spacy_nlp(joke)
        keywords = [token.lemma_ for token in doc if token.pos_ in ["NOUN", "ADJ", "VERB"]]
        return keywords[:10]  # Top 10
```

### 6.3. Router

Utw√≥rz `src/modules/joke_analyser/router.py`:

```python
"""
Router FastAPI dla serwisu Joke Analyser
"""

from fastapi import APIRouter, HTTPException, Depends
import logging
from .service import JokeAnalyserService
from .models import JokeAnalysisRequest, JokeAnalysisResponse
from api.config import config
from api.dependencies import get_logger

logger = get_logger(__name__)
router = APIRouter()

# Inicjalizacja serwisu
try:
    service = JokeAnalyserService()
    logger.info("‚úÖ Serwis Joke Analyser zainicjalizowany")
except Exception as e:
    logger.error(f"‚ùå B≈ÇƒÖd inicjalizacji serwisu Joke Analyser: {e}")
    service = None


@router.post("/analyze", response_model=JokeAnalysisResponse)
async def analyze_joke(
    request: JokeAnalysisRequest,
    logger: logging.Logger = Depends(get_logger)
):
    """
    Przeanalizuj ≈ºart u≈ºywajƒÖc r√≥≈ºnych technik analizy
    
    - **joke**: Tekst ≈ºartu do analizy (wymagane)
    - **techniques**: Lista technik analizy (opcjonalne, domy≈õlnie wszystkie)
    
    Dostƒôpne techniki:
    - incongruity
    - archetypes
    - psychoanalysis
    - setup_punchline
    - semantic_shift
    - absurd_escalation
    - timing
    - humor_micro_components
    - reverse_engineering
    """
    if service is None:
        raise HTTPException(
            status_code=503,
            detail="Serwis Joke Analyser nie jest dostƒôpny"
        )
    
    logger.info(f"Analiza ≈ºartu: {request.joke[:50]}...")
    return await service.analyze(request)


@router.get("/health")
async def health():
    """Health check dla serwisu Joke Analyser"""
    return {
        "status": "healthy" if service is not None else "unavailable",
        "model": config.JOKE_ANALYSER_MODEL_NAME if service else None,
        "available_techniques": service.AVAILABLE_TECHNIQUES if service else []
    }
```

---

## 7. KONFIGURACJA ≈öRODOWISKOWA

### 7.1. Plik .env

Utw√≥rz `.env` w katalogu g≈Ç√≥wnym projektu:

```bash
# Podstawowe ustawienia
SERVICE_NAME=ai-local-core
HOST=127.0.0.1
PORT=5001
DEBUG=false

# W≈ÇƒÖczanie/wy≈ÇƒÖczanie modu≈Ç√≥w
ENABLE_IMAGE_DESCRIPTION=true
ENABLE_OLLAMA=true
ENABLE_JOKER=false
ENABLE_JOKE_ANALYSER=false

# Image Description
IMAGE_MODEL_NAME=Salesforce/blip-image-captioning-base

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama2

# Joker (Bielik 7B)
JOKER_MODEL_NAME=piotradamczyk/bielik-7b-v0.1
JOKER_USE_GPU=true
JOKER_QUANTIZATION=int8

# Joke Analyser
JOKE_ANALYSER_MODEL_NAME=allegro/herbert-base-cased
JOKE_ANALYSER_USE_GPU=false
```

### 7.2. Konfiguracja dla r√≥≈ºnych ≈õrodowisk

**PC Ubuntu (joker):**
```bash
ENABLE_JOKER=true
ENABLE_JOKE_ANALYSER=false
PORT=5001
JOKER_USE_GPU=true
```

**M1 MacBook (analyser):**
```bash
ENABLE_JOKER=false
ENABLE_JOKE_ANALYSER=true
PORT=5002
JOKER_USE_GPU=false  # Dla analyser nie potrzebne
```

**Serwerownia (wszystko):**
```bash
ENABLE_JOKER=true
ENABLE_JOKE_ANALYSER=true
PORT=5001
JOKER_USE_GPU=true
```

---

## 8. TESTOWANIE

### 8.1. Uruchomienie serwera

```bash
# Z katalogu g≈Ç√≥wnego projektu
cd /Users/piotradamczyk/Projects/Octadecimal/ai-local-core
source venv/bin/activate

# Uruchom serwer
python -m src.api.main

# Lub przez uvicorn bezpo≈õrednio
uvicorn src.api.main:app --host 127.0.0.1 --port 5001 --reload
```

### 8.2. Testowanie endpoint√≥w

**Health check:**
```bash
curl http://127.0.0.1:5001/health
```

**Image Description:**
```bash
curl -X POST http://127.0.0.1:5001/describe/ \
  -H "Content-Type: application/json" \
  -d '{"image_url": "https://example.com/image.jpg", "max_length": 50}'
```

**Ollama Chat:**
```bash
curl -X POST http://127.0.0.1:5001/ollama/chat \
  -H "Content-Type: application/json" \
  -d '{"user": "Cze≈õƒá, jak siƒô masz?"}'
```

**Joker (je≈õli w≈ÇƒÖczony):**
```bash
curl -X POST http://127.0.0.1:5001/joker/generate \
  -H "Content-Type: application/json" \
  -d '{"topic": "programista", "style": "sarcastic", "length": "medium"}'
```

**Joke Analyser (je≈õli w≈ÇƒÖczony):**
```bash
curl -X POST http://127.0.0.1:5001/joke-analyser/analyze \
  -H "Content-Type: application/json" \
  -d '{"joke": "Dlaczego programista nie lubi natury? Bo ma za du≈ºo bug√≥w."}'
```

### 8.3. Dokumentacja API

FastAPI automatycznie generuje dokumentacjƒô:

- **Swagger UI:** http://127.0.0.1:5001/docs
- **ReDoc:** http://127.0.0.1:5001/redoc

### 8.4. Testy jednostkowe

Utw√≥rz testy w `tests/unit/`:

```python
# tests/unit/test_joker.py
import pytest
from modules.joker.service import JokerService
from modules.joker.models import JokeRequest

@pytest.mark.asyncio
async def test_joker_generate():
    service = JokerService()
    request = JokeRequest(topic="programista", style="sarcastic")
    response = await service.generate(request)
    assert response.success is True
    assert response.joke is not None
```

Uruchom testy:
```bash
pytest tests/unit/
```

---

## 9. DEPLOYMENT

### 9.1. Uruchomienie jako serwis systemowy

**Ubuntu (systemd):**

Utw√≥rz `/etc/systemd/system/ai-local-core.service`:

```ini
[Unit]
Description=AI Local Core FastAPI Service
After=network.target

[Service]
Type=simple
User=piotr
WorkingDirectory=/home/piotr/Projects/Octadecimal/ai-local-core
Environment="PATH=/home/piotr/Projects/Octadecimal/ai-local-core/venv/bin"
ExecStart=/home/piotr/Projects/Octadecimal/ai-local-core/venv/bin/uvicorn src.api.main:app --host 0.0.0.0 --port 5001
Restart=always

[Install]
WantedBy=multi-user.target
```

**Uruchomienie:**
```bash
sudo systemctl daemon-reload
sudo systemctl enable ai-local-core
sudo systemctl start ai-local-core
sudo systemctl status ai-local-core
```

### 9.2. Uruchomienie na M1 MacBook

**LaunchAgent (macOS):**

Utw√≥rz `~/Library/LaunchAgents/com.octadecimal.ai-local-core.plist`:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.octadecimal.ai-local-core</string>
    <key>ProgramArguments</key>
    <array>
        <string>/Users/piotradamczyk/Projects/Octadecimal/ai-local-core/venv/bin/uvicorn</string>
        <string>src.api.main:app</string>
        <string>--host</string>
        <string>0.0.0.0</string>
        <string>--port</string>
        <string>5002</string>
    </array>
    <key>WorkingDirectory</key>
    <string>/Users/piotradamczyk/Projects/Octadecimal/ai-local-core</string>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
</dict>
</plist>
```

**Uruchomienie:**
```bash
launchctl load ~/Library/LaunchAgents/com.octadecimal.ai-local-core.plist
launchctl start com.octadecimal.ai-local-core
```

### 9.3. Integracja z waldus-api (Laravel)

W `waldus-api/.env`:

```php
AI_LOCAL_CORE_URL=http://192.168.1.100:5001  # PC Ubuntu
AI_JOKE_ANALYSER_URL=http://192.168.1.101:5002  # M1 MacBook
```

W kontrolerze Laravel:

```php
// Przyk≈Çad u≈ºycia
$response = Http::post(config('services.ai_local_core.url') . '/joker/generate', [
    'topic' => 'programista',
    'style' => 'sarcastic'
]);
```

---

## 10. ROZW√ìJ DALSZYCH SERWIS√ìW

### 10.1. Wzorzec dodawania nowego serwisu

1. **Utw√≥rz strukturƒô katalog√≥w:**
```bash
mkdir -p src/modules/nazwa_serwisu
touch src/modules/nazwa_serwisu/{__init__.py,router.py,service.py,models.py}
```

2. **Zaimplementuj modele (models.py):**
```python
from pydantic import BaseModel

class RequestModel(BaseModel):
    field: str

class ResponseModel(BaseModel):
    success: bool
    result: str
```

3. **Zaimplementuj serwis (service.py):**
```python
class Service:
    async def process(self, request: RequestModel) -> ResponseModel:
        # Logika biznesowa
        pass
```

4. **Zaimplementuj router (router.py):**
```python
from fastapi import APIRouter
from .service import Service
from .models import RequestModel, ResponseModel

router = APIRouter()
service = Service()

@router.post("/endpoint", response_model=ResponseModel)
async def endpoint(request: RequestModel):
    return await service.process(request)
```

5. **Dodaj do config.py:**
```python
ENABLE_NAZWA_SERWISU: bool = False
```

6. **Dodaj do main.py:**
```python
if config.ENABLE_NAZWA_SERWISU:
    from modules.nazwa_serwisu.router import router as nazwa_router
    app.include_router(nazwa_router, prefix="/nazwa", tags=["Nazwa"])
```

7. **W≈ÇƒÖcz w .env:**
```bash
ENABLE_NAZWA_SERWISU=true
```

---

## 11. TROUBLESHOOTING

### 11.1. B≈ÇƒÖd: "Module not found"

**Problem:** Python nie znajduje modu≈Ç√≥w

**RozwiƒÖzanie:**
```bash
# Upewnij siƒô, ≈ºe jeste≈õ w katalogu g≈Ç√≥wnym projektu
cd /Users/piotradamczyk/Projects/Octadecimal/ai-local-core

# Sprawd≈∫ PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Lub uruchom przez modu≈Ç
python -m src.api.main
```

### 11.2. B≈ÇƒÖd: "Model not found"

**Problem:** Model Bielik/HerBERT nie zosta≈Ç pobrany

**RozwiƒÖzanie:**
```bash
# Dla Bielik (MLX)
python -c "from mlx_lm import load; load('piotradamczyk/bielik-7b-v0.1')"

# Dla HerBERT
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('allegro/herbert-base-cased')"
```

### 11.3. B≈ÇƒÖd: "GPU not available"

**Problem:** GPU nie jest dostƒôpne, ale `USE_GPU=true`

**RozwiƒÖzanie:**
```bash
# Sprawd≈∫ dostƒôpno≈õƒá GPU
python -c "import torch; print(torch.cuda.is_available())"

# Dla Mac (MPS)
python -c "import torch; print(torch.backends.mps.is_available())"

# Je≈õli GPU niedostƒôpne, ustaw w .env:
JOKER_USE_GPU=false
```

### 11.4. B≈ÇƒÖd: "Port already in use"

**Problem:** Port 5001 jest ju≈º zajƒôty

**RozwiƒÖzanie:**
```bash
# Znajd≈∫ proces u≈ºywajƒÖcy portu
lsof -i :5001

# Zabij proces
kill -9 <PID>

# Lub zmie≈Ñ port w .env
PORT=5002
```

---

## 12. CHECKLIST MIGRACJI

- [ ] Backup istniejƒÖcego kodu Flask
- [ ] Aktualizacja `requirements.txt` (FastAPI, uvicorn, pydantic)
- [ ] Instalacja zale≈ºno≈õci (`pip install -r requirements.txt`)
- [ ] Utworzenie struktury katalog√≥w modularnej
- [ ] Utworzenie `config.py` z konfiguracjƒÖ
- [ ] Utworzenie `main.py` z FastAPI app
- [ ] Migracja endpoint√≥w Image Description
- [ ] Migracja endpoint√≥w Ollama
- [ ] Testowanie istniejƒÖcych endpoint√≥w
- [ ] Implementacja serwisu ai-joker
- [ ] Implementacja serwisu ai-joke-analyser
- [ ] Konfiguracja `.env` dla r√≥≈ºnych ≈õrodowisk
- [ ] Testy jednostkowe
- [ ] Dokumentacja API (automatyczna przez FastAPI)
- [ ] Deployment na PC Ubuntu
- [ ] Deployment na M1 MacBook
- [ ] Integracja z waldus-api (Laravel)
- [ ] Monitoring i logi

---

## 13. DODATKOWE ZASOBY

### Dokumentacja:
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [Uvicorn Documentation](https://www.uvicorn.org/)
- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)

### Modele:
- [Bielik 7B v0.1](https://huggingface.co/piotradamczyk/bielik-7b-v0.1)
- [HerBERT](https://huggingface.co/allegro/herbert-base-cased)
- [spaCy Polish](https://spacy.io/models/pl)

### Architektura:
- Zobacz: `waldus-api/docs/analysis/architektura-rozproszona-realna-sytuacja.md`

---

**Koniec instrukcji migracji i rozwoju serwis√≥w AI**

