# WL-2: Provider Selection ML ğŸ¤–

**â† PowrÃ³t do:** [waldus-api WL-2 Faza 4](../../../waldus-api/docs/features/WL-2-faza-4-provider-selection.md)

---

## ğŸ“‹ Informacje podstawowe

**Repozytorium:** `ai-local-core`  
**Czas implementacji:** 1-2 tygodnie  
**Priorytet:** â­â­ (niski, ale duÅ¼y potencjaÅ‚)  
**ZaleÅ¼noÅ›ci:** Zebranie danych (min. 1000+ interakcji), waldus-api Faza 4.1

---

## ğŸ¯ Cel

Zastosowanie Machine Learning do inteligentnego wyboru najlepszego providera LLM na podstawie:
- Historycznych metryk wydajnoÅ›ci
- Ocen uÅ¼ytkownikÃ³w
- Kontekstu uÅ¼ytkownika i promptu
- Samonauki w czasie rzeczywistym (Reinforcement Learning)

---

## ğŸ—ï¸ Architektura

### Umieszczenie ML w ai-local-core

**Uzasadnienie:**
- âœ… **Lepsze biblioteki ML:** Python ma bogaty ekosystem (scikit-learn, PyTorch, TensorFlow, XGBoost)
- âœ… **JuÅ¼ istniejÄ…ca infrastruktura:** Flask API, Å‚atwe dodanie endpointu `/select-provider`
- âœ… **Izolacja:** ML model nie obciÄ…Å¼a gÅ‚Ã³wnej aplikacji PHP
- âœ… **Skalowanie:** MoÅ¼na uruchomiÄ‡ osobny serwer ML bez wpÅ‚ywu na waldus-api
- âœ… **Trening modelu:** Åatwiejsze w Pythonie (pandas, numpy, scikit-learn)

### Komunikacja

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         HTTP POST          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   waldus-api    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   â”‚  ai-local-core   â”‚
â”‚                 â”‚                             â”‚                  â”‚
â”‚ ProviderSelectionâ”‚  {                         â”‚   ML Model       â”‚
â”‚    Service      â”‚    waldus_uuid,             â”‚  (Python)        â”‚
â”‚    (PHP)        â”‚    user_context,            â”‚                  â”‚
â”‚                 â”‚    provider_metrics,        â”‚  /select-providerâ”‚
â”‚                 â”‚    priority                 â”‚  /update-reward  â”‚
â”‚                 â”‚  }                          â”‚                  â”‚
â”‚                 â”‚                             â”‚                  â”‚
â”‚                 â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚                  â”‚
â”‚                 â”‚  {                         â”‚                  â”‚
â”‚                 â”‚    provider: "anthropic",  â”‚                  â”‚
â”‚                 â”‚    confidence: 0.85        â”‚                  â”‚
â”‚                 â”‚  }                         â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Struktura projektu

```
ai-local-core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ml/                          # NOWY MODUÅ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ provider_selector.py    # GÅ‚Ã³wna logika ML
â”‚   â”‚   â”œâ”€â”€ features.py              # Feature engineering
â”‚   â”‚   â”œâ”€â”€ models/                  # Zapisane modele
â”‚   â”‚   â”‚   â””â”€â”€ bandit_state.json   # Stan Multi-Armed Bandit
â”‚   â”‚   â””â”€â”€ training/                # Skrypty treningowe
â”‚   â”‚       â”œâ”€â”€ train_bandit.py
â”‚   â”‚       â”œâ”€â”€ train_xgboost.py
â”‚   â”‚       â””â”€â”€ evaluate.py
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ server.py                # Flask API (rozszerzyÄ‡)
```

---

## ğŸ¤– Propozycje modeli ML

### Opcja A: Reinforcement Learning (Multi-Armed Bandit) â­ REKOMENDOWANE

**Algorytm:** Thompson Sampling

**Zalety:**
- âœ… Uczy siÄ™ w czasie rzeczywistym
- âœ… Nie wymaga danych treningowych
- âœ… Automatycznie adaptuje siÄ™ do zmian
- âœ… Prosty w implementacji

**Wady:**
- âŒ Wymaga czasu na "rozgrzanie"
- âŒ MoÅ¼e eksperymentowaÄ‡

**Implementacja:**

```python
# src/ml/provider_selector.py

import numpy as np
from scipy.stats import beta
import json
import os
from typing import Dict, Tuple

class RLProviderSelector:
    """
    Multi-Armed Bandit z Thompson Sampling dla wyboru providera
    """
    
    def __init__(self, state_file='src/ml/models/bandit_state.json'):
        self.state_file = state_file
        
        # Dla kaÅ¼dego providera: alpha (sukcesy), beta (poraÅ¼ki)
        self.arms = {
            'anthropic': {'alpha': 1.0, 'beta': 1.0},  # Uniform prior
            'openai': {'alpha': 1.0, 'beta': 1.0},
            'groq': {'alpha': 1.0, 'beta': 1.0},
            'ollama': {'alpha': 1.0, 'beta': 1.0},
            'deepseek': {'alpha': 1.0, 'beta': 1.0},
        }
        
        # Wczytaj zapisany stan
        self.load_state()
    
    def select_provider(self, waldus_uuid: str, context: dict = None) -> Tuple[str, float]:
        """
        Thompson Sampling: wybierz providera na podstawie beta distribution
        
        Returns:
            (provider_name, confidence)
        """
        samples = {}
        
        for provider, params in self.arms.items():
            # PrÃ³bkuj z beta distribution
            sample = beta.rvs(params['alpha'], params['beta'])
            samples[provider] = sample
        
        # Wybierz providera z najwyÅ¼szym sample
        selected = max(samples, key=samples.get)
        confidence = samples[selected]
        
        return selected, confidence
    
    def update_reward(
        self,
        waldus_uuid: str,
        provider: str,
        reward: float  # 0-1 (normalizowana ocena uÅ¼ytkownika)
    ):
        """
        Aktualizuj statystyki po otrzymaniu feedbacku
        reward: 0-1 (np. (rating - 1) / 4 dla rating 1-5)
        """
        if provider not in self.arms:
            return
        
        # Aktualizuj alpha (sukcesy) i beta (poraÅ¼ki)
        # reward bliskie 1.0 â†’ sukces, bliskie 0.0 â†’ poraÅ¼ka
        self.arms[provider]['alpha'] += reward
        self.arms[provider]['beta'] += (1 - reward)
        
        # Zapisz stan
        self.save_state()
    
    def get_stats(self) -> Dict:
        """
        Zwraca statystyki dla wszystkich providerÃ³w
        """
        stats = {}
        
        for provider, params in self.arms.items():
            # Åšrednia beta distribution = alpha / (alpha + beta)
            mean = params['alpha'] / (params['alpha'] + params['beta'])
            
            # Liczba prÃ³b
            total_trials = params['alpha'] + params['beta'] - 2  # Odejmij priory
            
            stats[provider] = {
                'mean': mean,
                'alpha': params['alpha'],
                'beta': params['beta'],
                'total_trials': total_trials,
            }
        
        return stats
    
    def save_state(self):
        """Zapisz stan do pliku"""
        os.makedirs(os.path.dirname(self.state_file), exist_ok=True)
        
        with open(self.state_file, 'w') as f:
            json.dump(self.arms, f, indent=2)
    
    def load_state(self):
        """Wczytaj stan z pliku"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r') as f:
                    saved_arms = json.load(f)
                    
                # Aktualizuj tylko istniejÄ…ce armes
                for provider, params in saved_arms.items():
                    if provider in self.arms:
                        self.arms[provider] = params
                        
            except Exception as e:
                print(f"Warning: Could not load state: {e}")
```

### Opcja B: XGBoost (Gradient Boosting)

**Dla zaawansowanego uÅ¼ycia po zebraniu danych (1000+ interakcji)**

```python
# src/ml/training/train_xgboost.py

import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
import joblib

class XGBoostProviderSelector:
    """
    XGBoost dla wyboru providera na podstawie zebranych danych
    """
    
    def __init__(self, model_path='src/ml/models/xgboost_provider.model'):
        self.model_path = model_path
        self.model = None
        self.load_model()
    
    def train(self, training_data: pd.DataFrame):
        """
        Trenuj model na zebranych danych
        
        training_data columns:
        - provider (target)
        - avg_latency_ms
        - success_rate
        - avg_rating
        - user_age
        - user_humor_style
        - prompt_complexity
        - priority
        """
        # Przygotuj features
        X = training_data.drop(['provider'], axis=1)
        y = training_data['provider'].astype('category').cat.codes
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Train
        self.model = xgb.XGBClassifier(
            objective='multi:softmax',
            num_class=len(training_data['provider'].unique()),
            max_depth=5,
            learning_rate=0.1,
            n_estimators=100
        )
        
        self.model.fit(X_train, y_train)
        
        # Evaluate
        accuracy = self.model.score(X_test, y_test)
        print(f"Model accuracy: {accuracy:.2%}")
        
        # Save
        self.save_model()
    
    def predict(self, features: dict) -> Tuple[str, float]:
        """
        Przewiduj najlepszego providera
        """
        if self.model is None:
            return 'anthropic', 0.5  # Default
        
        # Przygotuj features
        X = pd.DataFrame([features])
        
        # Predict
        provider_code = self.model.predict(X)[0]
        proba = self.model.predict_proba(X)[0]
        
        # Map code to provider name
        provider_names = ['anthropic', 'openai', 'groq', 'ollama', 'deepseek']
        provider = provider_names[provider_code]
        confidence = proba[provider_code]
        
        return provider, confidence
    
    def save_model(self):
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
        joblib.dump(self.model, self.model_path)
    
    def load_model(self):
        if os.path.exists(self.model_path):
            self.model = joblib.load(self.model_path)
```

---

## ğŸŒ API Endpoints

### Dokumentacja API: [provider-selection.md](../api/provider-selection.md)

### POST /select-provider

**Wybiera najlepszego providera**

**Request:**
```json
{
  "waldus_uuid": "abc-123",
  "user_context": {
    "age": 25,
    "humor_style": "sarcastic",
    "engagement_level": "high"
  },
  "prompt_context": {
    "page_type": "blog",
    "complexity": 150
  },
  "provider_metrics": {
    "anthropic": {"avg_latency_ms": 500, "success_rate": 0.98},
    "openai": {"avg_latency_ms": 800, "success_rate": 0.95}
  },
  "priority": 10
}
```

**Response:**
```json
{
  "provider": "anthropic",
  "confidence": 0.85,
  "reason": "Best combination of latency and historical success rate"
}
```

### POST /update-reward

**Aktualizuje statystyki po otrzymaniu feedbacku**

**Request:**
```json
{
  "waldus_uuid": "abc-123",
  "provider": "anthropic",
  "reward": 0.875
}
```

**Response:**
```json
{
  "success": true,
  "updated_stats": {
    "anthropic": {
      "mean": 0.85,
      "total_trials": 42
    }
  }
}
```

### GET /provider-stats

**Zwraca statystyki wszystkich providerÃ³w**

**Response:**
```json
{
  "anthropic": {
    "mean": 0.85,
    "alpha": 34.5,
    "beta": 6.2,
    "total_trials": 40
  },
  "openai": {
    "mean": 0.78,
    "alpha": 28.3,
    "beta": 8.1,
    "total_trials": 36
  }
}
```

---

## ğŸ”§ Implementacja w Flask API

**Plik:** `src/api/server.py`

```python
from flask import Flask, request, jsonify
from src.ml.provider_selector import RLProviderSelector

app = Flask(__name__)

# Inicjalizuj selector
selector = RLProviderSelector()

@app.route('/select-provider', methods=['POST'])
def select_provider():
    """
    Wybiera najlepszego providera na podstawie ML modelu
    """
    data = request.json
    
    waldus_uuid = data.get('waldus_uuid')
    user_context = data.get('user_context', {})
    prompt_context = data.get('prompt_context', {})
    provider_metrics = data.get('provider_metrics', {})
    priority = data.get('priority', 10)
    
    # Wybierz providera
    provider, confidence = selector.select_provider(
        waldus_uuid,
        context={
            'user': user_context,
            'prompt': prompt_context,
            'metrics': provider_metrics,
            'priority': priority
        }
    )
    
    return jsonify({
        'provider': provider,
        'confidence': float(confidence),
        'reason': f'Thompson Sampling confidence: {confidence:.2%}'
    })

@app.route('/update-reward', methods=['POST'])
def update_reward():
    """
    Aktualizuje statystyki po otrzymaniu feedbacku
    """
    data = request.json
    
    waldus_uuid = data.get('waldus_uuid')
    provider = data.get('provider')
    reward = data.get('reward')  # 0-1
    
    if not all([waldus_uuid, provider, reward is not None]):
        return jsonify({'error': 'Missing required fields'}), 400
    
    # Aktualizuj reward
    selector.update_reward(waldus_uuid, provider, reward)
    
    # ZwrÃ³Ä‡ zaktualizowane statystyki
    stats = selector.get_stats()
    
    return jsonify({
        'success': True,
        'updated_stats': stats
    })

@app.route('/provider-stats', methods=['GET'])
def get_provider_stats():
    """
    Zwraca statystyki wszystkich providerÃ³w
    """
    stats = selector.get_stats()
    return jsonify(stats)

# Existing endpoints...
# /describe, /health, etc.
```

---

## ğŸ“¦ Instalacja zaleÅ¼noÅ›ci

**Plik:** `requirements.txt`

```txt
# Existing dependencies
flask>=2.3.0
Pillow>=10.0.0
transformers>=4.30.0
torch>=2.0.0
requests>=2.31.0

# NEW: ML dependencies
scipy>=1.11.0
scikit-learn>=1.3.0
xgboost>=2.0.0
pandas>=2.0.0
numpy>=1.24.0
joblib>=1.3.0
```

**Instalacja:**

```bash
cd /Users/piotradamczyk/Projects/Octadecimal/ai-local-core
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ§ª Testy

**Plik:** `tests/unit/test_provider_selector.py`

```python
import pytest
from src.ml.provider_selector import RLProviderSelector

def test_select_provider():
    selector = RLProviderSelector()
    
    provider, confidence = selector.select_provider('test-uuid')
    
    assert provider in ['anthropic', 'openai', 'groq', 'ollama', 'deepseek']
    assert 0 <= confidence <= 1

def test_update_reward():
    selector = RLProviderSelector()
    
    # High reward
    selector.update_reward('test-uuid', 'anthropic', 0.9)
    
    stats = selector.get_stats()
    assert stats['anthropic']['alpha'] > 1.0  # Increased

def test_thompson_sampling_converges():
    selector = RLProviderSelector()
    
    # Symuluj wiele nagrÃ³d dla anthropic
    for _ in range(100):
        selector.update_reward('test-uuid', 'anthropic', 0.9)
    
    # Anthropic powinien mieÄ‡ najwyÅ¼szy mean
    stats = selector.get_stats()
    anthropic_mean = stats['anthropic']['mean']
    
    for provider, provider_stats in stats.items():
        if provider != 'anthropic':
            assert anthropic_mean > provider_stats['mean']
```

---

## ğŸ“Š Monitoring

**Dashboard do wizualizacji:**

```python
# src/ml/dashboard.py (opcjonalnie)

import matplotlib.pyplot as plt
from src.ml.provider_selector import RLProviderSelector

def plot_provider_stats():
    selector = RLProviderSelector()
    stats = selector.get_stats()
    
    providers = list(stats.keys())
    means = [stats[p]['mean'] for p in providers]
    trials = [stats[p]['total_trials'] for p in providers]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Mean confidence
    ax1.bar(providers, means)
    ax1.set_title('Provider Mean Confidence')
    ax1.set_ylabel('Mean')
    
    # Total trials
    ax2.bar(providers, trials)
    ax2.set_title('Provider Total Trials')
    ax2.set_ylabel('Trials')
    
    plt.tight_layout()
    plt.savefig('provider_stats.png')
```

---

## ğŸš€ Deployment

**Uruchomienie serwera:**

```bash
cd /Users/piotradamczyk/Projects/Octadecimal/ai-local-core
source venv/bin/activate
python src/api/server.py
```

**Lub jako serwis systemd:**

```ini
# /etc/systemd/system/ai-local-core-ml.service

[Unit]
Description=AI Local Core ML Service
After=network.target

[Service]
Type=simple
User=piotradamczyk
WorkingDirectory=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core
ExecStart=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core/venv/bin/python src/api/server.py
Restart=always

[Install]
WantedBy=multi-user.target
```

---

## ğŸ“ NastÄ™pne kroki

1. âœ… ZaimplementowaÄ‡ `RLProviderSelector` w `src/ml/provider_selector.py`
2. âœ… DodaÄ‡ endpointy do `src/api/server.py`
3. âœ… DodaÄ‡ testy w `tests/unit/test_provider_selector.py`
4. â³ ZebraÄ‡ dane (1000+ interakcji) z waldus-api
5. â³ ZaimplementowaÄ‡ `XGBoostProviderSelector` (opcjonalnie)
6. â³ A/B testing: RL vs XGBoost vs heurystyki

---

**Status:** ğŸŸ¡ Gotowy do implementacji  
**Ostatnia aktualizacja:** 2025-11-10

