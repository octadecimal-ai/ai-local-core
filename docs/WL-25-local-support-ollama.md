# WL-25: ai-local-core - Self-hosted Ollama na RTX 3060 ğŸ–¥ï¸

## ğŸ“‹ Opis zadania

Utworzenie lokalnego wsparcia dla Waldus API poprzez uruchomienie wÅ‚asnego LLM (Ollama) na dedykowanym PC z RTX 3060 12GB. Projekt ma na celu zapewnienie fallbacku dla zewnÄ™trznych API LLM oraz peÅ‚nej kontroli nad danymi i kosztami.

---

## ğŸ–¥ï¸ Specyfikacja sprzÄ™tu

**CPU:** AMD Ryzen 5 5600X (6 cores, 12 threads)  
**RAM:** 16 GB  
**GPU:** NVIDIA GeForce RTX 3060 (12 GB VRAM)  
**System:** Ubuntu Server  
**Internet:** ÅšwiatÅ‚owÃ³d UPC/Play (staÅ‚e IP, ale problemy z publicznym dostÄ™pem)  
**RozwiÄ…zanie tunelu:** ngrok (do zmiany na Cloudflare Tunnel lub Tailscale)

---

## ğŸ” Analiza moÅ¼liwoÅ›ci

### âœ… DoskonaÅ‚a konfiguracja dla Ollama!

**RTX 3060 12GB VRAM** to idealna karta dla lokalnego LLM:
- WystarczajÄ…ca VRAM dla wiÄ™kszoÅ›ci modeli 7B-13B w wersji quantized
- CUDA acceleration zapewnia wysokÄ… wydajnoÅ›Ä‡
- 12GB VRAM pozwala na uruchomienie wiÄ™kszych modeli niÅ¼ na CPU

### Modele moÅ¼liwe do uruchomienia

#### Modele 7B (Q4/Q5) - âœ… Idealne dla RTX 3060

| Model | VRAM | RAM | WydajnoÅ›Ä‡ (RTX 3060) | JakoÅ›Ä‡ |
|-------|------|-----|---------------------|--------|
| **Llama 3.1 8B Q4** | ~6-7 GB | ~8 GB | ~30-50 tok/s | â­â­â­â­â­ |
| **Mistral 7B Q4** | ~5-6 GB | ~7 GB | ~35-55 tok/s | â­â­â­â­â­ |
| **Llama 3.2 7B Q4** | ~6-7 GB | ~8 GB | ~30-50 tok/s | â­â­â­â­â­ |
| **Phi-3 Medium 3.8B** | ~3-4 GB | ~5 GB | ~60-80 tok/s | â­â­â­â­ |
| **Gemma 2 9B Q4** | ~7-8 GB | ~9 GB | ~25-40 tok/s | â­â­â­â­â­ |

#### Modele 13B (Q4) - âš ï¸ MoÅ¼liwe, ale na granicy

| Model | VRAM | RAM | WydajnoÅ›Ä‡ | Status |
|-------|------|-----|-----------|--------|
| **Llama 3.1 13B Q4** | ~9-10 GB | ~12 GB | ~15-25 tok/s | âš ï¸ MoÅ¼liwe, ale wolne |
| **Mistral 13B Q4** | ~9-10 GB | ~12 GB | ~15-25 tok/s | âš ï¸ MoÅ¼liwe, ale wolne |

**Wnioski:**
- âœ… **Rekomendowane:** Modele 7B-9B w Q4 - doskonaÅ‚a wydajnoÅ›Ä‡ (30-50 tok/s)
- âš ï¸ **MoÅ¼liwe:** Modele 13B w Q4 - wolniejsze (15-25 tok/s), ale dziaÅ‚ajÄ…
- âŒ **Niewskazane:** Modele >13B lub FP16 - nie zmieszczÄ… siÄ™ w 12GB VRAM

### WydajnoÅ›Ä‡ vs. API

| Provider | WydajnoÅ›Ä‡ | Koszt | Status |
|----------|-----------|-------|--------|
| **RTX 3060 (Llama 3.1 8B Q4)** | ~30-50 tok/s | 0 PLN (energia) | âœ… DoskonaÅ‚e! |
| **Anthropic Claude** | ~50-100 tok/s | Pay-per-use | âš ï¸ Kosztowne |
| **Groq API** | ~500+ tok/s | Darmowy tier | âš ï¸ ZewnÄ™trzne API |
| **OpenAI GPT-3.5** | ~50-100 tok/s | Pay-per-use | âš ï¸ Kosztowne |

**Wnioski:**
- RTX 3060 daje **porÃ³wnywalnÄ… wydajnoÅ›Ä‡** do API (30-50 tok/s vs. 50-100 tok/s)
- **Zero kosztÃ³w** (poza energiÄ… elektrycznÄ… ~50-100 PLN/mies.)
- **PeÅ‚na prywatnoÅ›Ä‡** danych
- **Brak limitÃ³w API**

---

## ğŸŒ Problemy z publicznym IP i rozwiÄ…zania

### Problem: StaÅ‚e IP bez publicznego dostÄ™pu

**MoÅ¼liwe przyczyny:**
1. **CGNAT (Carrier-Grade NAT)** - Play/UPC uÅ¼ywa NAT, wiÄ™c IP nie jest publiczne
2. **Firewall ISP** - blokada portÃ³w przychodzÄ…cych
3. **Router configuration** - brak port forwarding

### RozwiÄ…zanie 1: ngrok âš ï¸ (obecne)

**Zalety:**
- âœ… Szybka konfiguracja
- âœ… DziaÅ‚a od razu
- âœ… Darmowy tier dostÄ™pny

**Wady:**
- âŒ **Limit darmowego tier:** 40 connections/min, 2GB transfer/mies.
- âŒ **Timeout:** poÅ‚Ä…czenia mogÄ… siÄ™ rozÅ‚Ä…czaÄ‡
- âŒ **BezpieczeÅ„stwo:** dane przechodzÄ… przez serwery ngrok
- âŒ **NiezawodnoÅ›Ä‡:** moÅ¼e byÄ‡ niestabilne dla produkcyjnego uÅ¼ycia
- âŒ **Koszty:** pÅ‚atny plan ($8/mies.) dla wiÄ™kszego ruchu

### RozwiÄ…zanie 2: Cloudflare Tunnel (Cloudflared) â­ (REKOMENDOWANE)

**Zalety:**
- âœ… **Darmowe** - bez limitÃ³w transferu
- âœ… **Bezpieczne** - end-to-end encryption
- âœ… **Niezawodne** - stabilne poÅ‚Ä…czenia
- âœ… **Szybsze** - lepsza wydajnoÅ›Ä‡ niÅ¼ ngrok
- âœ… **Darmowa domena** - moÅ¼liwoÅ›Ä‡ uÅ¼ycia darmowej domeny Cloudflare

**Wady:**
- âš ï¸ Wymaga rejestracji w Cloudflare (darmowe)
- âš ï¸ Konfiguracja nieco bardziej zÅ‚oÅ¼ona niÅ¼ ngrok

**Instalacja:**
```bash
# Pobierz cloudflared
wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
chmod +x cloudflared-linux-amd64
sudo mv cloudflared-linux-amd64 /usr/local/bin/cloudflared

# Uruchom tunnel
cloudflared tunnel --url http://localhost:11434
```

### RozwiÄ…zanie 3: Tailscale / ZeroTier (VPN Mesh) â­â­ (NAJLEPSZE dla bezpieczeÅ„stwa)

**Zalety:**
- âœ… **Najbezpieczniejsze** - VPN mesh, end-to-end encryption
- âœ… **Darmowe** - do 100 urzÄ…dzeÅ„
- âœ… **Niezawodne** - stabilne poÅ‚Ä…czenia
- âœ… **Proste** - Å‚atwa konfiguracja
- âœ… **Bez poÅ›rednikÃ³w** - bezpoÅ›rednie poÅ‚Ä…czenie P2P

**Wady:**
- âš ï¸ Wymaga instalacji na obu koÅ„cach (serwer + klient API)

**Instalacja:**
```bash
# Tailscale
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up

# ZeroTier
curl -s https://install.zerotier.com | sudo bash
sudo zerotier-cli join <network-id>
```

### RozwiÄ…zanie 4: WireGuard VPN (zaawansowane)

**Zalety:**
- âœ… **Najszybsze** - niski overhead
- âœ… **Bezpieczne** - nowoczesna kryptografia
- âœ… **Darmowe** - open source

**Wady:**
- âŒ Wymaga wÅ‚asnego serwera VPN (moÅ¼na uÅ¼yÄ‡ VPS)
- âŒ Konfiguracja bardziej zÅ‚oÅ¼ona

---

## ğŸ—ï¸ Architektura rozwiÄ…zania

### Opcja A: Cloudflare Tunnel (dla szybkiego startu) â­

```
[Waldus API] â†’ [Cloudflare Tunnel] â†’ [Ollama na PC (localhost:11434)]
```

**Kroki:**
1. Zainstaluj Ollama na Ubuntu Server
2. Skonfiguruj Ollama API (domyÅ›lnie port 11434)
3. Uruchom Cloudflare Tunnel: `cloudflared tunnel --url http://localhost:11434`
4. Uzyskaj publiczny URL (np. `https://ollama-xxx.trycloudflare.com`)
5. Skonfiguruj FallbackService w Waldus API do uÅ¼ywania tego URL

**Zalety:**
- âœ… Szybka konfiguracja (15 minut)
- âœ… Darmowe
- âœ… WystarczajÄ…ce dla fazy 1-2

### Opcja B: Tailscale (dla produkcyjnego uÅ¼ycia) â­â­

```
[Waldus API] â†’ [Tailscale VPN] â†’ [Ollama na PC (tailscale-ip:11434)]
```

**Kroki:**
1. Zainstaluj Tailscale na Ubuntu Server
2. Zainstaluj Tailscale na serwerze Waldus API (lub uÅ¼yj Tailscale w Docker)
3. Skonfiguruj Ollama do nasÅ‚uchiwania na IP Tailscale
4. Skonfiguruj FallbackService do uÅ¼ywania Tailscale IP

**Zalety:**
- âœ… Najbezpieczniejsze
- âœ… Najbardziej niezawodne
- âœ… Zero kosztÃ³w
- âœ… Idealne dla produkcyjnego uÅ¼ycia

---

## ğŸ”’ BezpieczeÅ„stwo Ollama API

### Problem: Ollama domyÅ›lnie nie ma autoryzacji

**Ryzyka:**
- âŒ KaÅ¼dy z dostÄ™pem do URL moÅ¼e uÅ¼ywaÄ‡ Ollama
- âŒ MoÅ¼liwoÅ›Ä‡ naduÅ¼yÄ‡ (wysokie koszty energii)
- âŒ Brak rate limiting

### RozwiÄ…zanie: Reverse Proxy z autoryzacjÄ…

**Nginx + Basic Auth:**

```nginx
server {
    listen 11434;
    server_name ollama.local;

    location / {
        auth_basic "Ollama API";
        auth_basic_user_file /etc/nginx/.htpasswd;
        
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

**Lub uÅ¼yj Ollama z API key (jeÅ›li dostÄ™pne w przyszÅ‚oÅ›ci)**

---

## ğŸ¯ Plan implementacji

### Faza 1: Setup podstawowy (2-3 godziny) âš¡

1. âœ… Zainstaluj Ollama na Ubuntu Server
2. âœ… Zainstaluj wybrany model (Llama 3.1 8B Q4 lub Mistral 7B Q4)
3. âœ… Przetestuj lokalnie: `curl http://localhost:11434/api/generate`
4. âœ… Skonfiguruj Cloudflare Tunnel dla szybkiego startu

### Faza 2: BezpieczeÅ„stwo (1-2 godziny) ğŸ”’

1. âœ… Skonfiguruj Nginx reverse proxy z Basic Auth
2. âœ… Przetestuj dostÄ™p przez tunnel
3. âœ… Skonfiguruj firewall (ufw) - tylko porty potrzebne

### Faza 3: Integracja z Waldus API (2-3 godziny) ğŸ”—

1. âœ… UtwÃ³rz OllamaProvider w Waldus API
2. âœ… Zintegruj z FallbackService
3. âœ… Dodaj monitoring i logowanie
4. âœ… Przetestuj fallback (wyÅ‚Ä…cz Anthropic, sprawdÅº czy uÅ¼ywa Ollama)

### Faza 4: Optymalizacja (opcjonalnie) ğŸš€

1. âš ï¸ RozwaÅ¼ Tailscale zamiast Cloudflare Tunnel (dla produkcyjnego)
2. âš ï¸ Dodaj rate limiting
3. âš ï¸ Monitoruj zuÅ¼ycie energii
4. âš ï¸ RozwaÅ¼ automatyczne wyÅ‚Ä…czanie GPU gdy nieuÅ¼ywane

---

## ğŸ’° Koszty vs. korzyÅ›ci

| Aspekt | Self-hosted (RTX 3060) | API (Anthropic/Groq) |
|--------|------------------------|---------------------|
| **Koszt miesiÄ™czny** | ~50-100 PLN (energia) | ~200-500 PLN (API) |
| **WydajnoÅ›Ä‡** | 30-50 tok/s | 50-500 tok/s |
| **PrywatnoÅ›Ä‡** | âœ… PeÅ‚na | âŒ Dane u providera |
| **NiezaleÅ¼noÅ›Ä‡** | âœ… Brak limitÃ³w | âš ï¸ Limity API |
| **ZÅ‚oÅ¼onoÅ›Ä‡** | âš ï¸ Wymaga zarzÄ…dzania | âœ… Zero zarzÄ…dzania |
| **NiezawodnoÅ›Ä‡** | âš ï¸ ZaleÅ¼y od PC/Internetu | âœ… Wysoka |

**Wnioski:**
- âœ… **OszczÄ™dnoÅ›Ä‡:** ~100-400 PLN/mies. (vs. API)
- âœ… **WydajnoÅ›Ä‡:** WystarczajÄ…ca dla fallbacku (30-50 tok/s)
- âœ… **PrywatnoÅ›Ä‡:** PeÅ‚na kontrola
- âš ï¸ **ZÅ‚oÅ¼onoÅ›Ä‡:** Wymaga zarzÄ…dzania, ale warto

---

## âœ… Finalna rekomendacja

**âœ… ZDECYDOWANIE TAK - Self-hosted Ollama na RTX 3060**

**Plan dziaÅ‚ania:**
1. **Teraz:** Zainstaluj Ollama + Cloudflare Tunnel (2-3h)
2. **Faza 1:** UÅ¼yj jako fallback w kontekstowym fallbacku (#1 z WL-2)
3. **Faza 3:** Zintegruj z FallbackService jako alternatywa dla Anthropic
4. **DÅ‚ugoterminowo:** RozwaÅ¼ Tailscale zamiast ngrok dla lepszego bezpieczeÅ„stwa

**Rekomendowany model:** Llama 3.1 8B Q4 lub Mistral 7B Q4 (najlepsza jakoÅ›Ä‡ przy 30-50 tok/s)

---

## ğŸ“ Notatki implementacyjne

### Status Ollama
- âœ… Ollama jest juÅ¼ zainstalowana na Ubuntu Server
- âœ… Ollama jest zainstalowana na MacOS (MacBook Pro M1) - wersja 0.11.6
- â³ Do zrobienia: Konfiguracja tunelu, bezpieczeÅ„stwo, integracja

### ZaleÅ¼noÅ›ci z innymi zadaniami
- **WL-2:** Dopracowanie fallbacku - uÅ¼ywa tego rozwiÄ…zania jako fallback dla LLMService
- **WL-79:** Integracja Jira-Cursor - moÅ¼e uÅ¼ywaÄ‡ do zarzÄ…dzania zadaniami

---

## ğŸš€ Plan dziaÅ‚ania - Development na MacOS, Production na Ubuntu Server

### ğŸ“ Kontekst projektu

**Åšrodowisko development:**
- **System:** MacOS Sonoma (MacBook Pro M1)
- **Ollama:** Zainstalowana (wersja 0.11.6), ale nieuruchomiona
- **Cel:** Utworzenie repozytorium i rozwÃ³j projektu lokalnie

**Åšrodowisko production:**
- **System:** Ubuntu Server (PC z RTX 3060)
- **Ollama:** Do zainstalowania/konfiguracji
- **Cel:** DziaÅ‚ajÄ…cy serwer LLM dla Waldus API

### ğŸ¯ Faza 0: Setup repozytorium na MacOS (1-2 godziny) ğŸ

**Cel:** Przygotowanie struktury projektu do rozwoju na MacOS i migracja Pythona z waldus-api

#### 1. Struktura repozytorium
```
ai-local-core/
â”œâ”€â”€ .dev/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ time.sh
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ cursor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ollama/                  # Komunikacja z Ollama API (Python)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ client.py            # Nowa klasa OllamaClient
â”‚   â”‚   â”œâ”€â”€ complete.py           # ollama_complete.py (przeniesiony z waldus-api)
â”‚   â”‚   â”œâ”€â”€ models.py            # Modele danych (Pydantic)
â”‚   â”‚   â”œâ”€â”€ exceptions.py        # WyjÄ…tki
â”‚   â”‚   â””â”€â”€ utils.py             # Helper functions
â”‚   â”œâ”€â”€ image/                   # Rozpoznawanie obrazkÃ³w (Python)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ describe.py          # describe_image.py (przeniesiony z waldus-api)
â”‚   â”‚   â””â”€â”€ models.py            # Modele BLIP
â”‚   â”œâ”€â”€ translation/             # TÅ‚umaczenie tekstu (Python)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ translate.py         # translate_text.py (przeniesiony z waldus-api)
â”‚   â”‚   â””â”€â”€ models.py            # Modele tÅ‚umaczenia
â”‚   â”œâ”€â”€ api/                      # Flask API server (Python)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ server.py            # api_server.py (przeniesiony z waldus-api)
â”‚   â”œâ”€â”€ tunnel-manager/          # ZarzÄ…dzanie tunelami (Cloudflare/Tailscale) - Python
â”‚   â”œâ”€â”€ security/                # Reverse proxy, auth, rate limiting - Python
â”‚   â””â”€â”€ monitoring/              # Monitoring i logowanie - Python
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ development.yaml         # Konfiguracja dla MacOS
â”‚   â”œâ”€â”€ production.yaml          # Konfiguracja dla Ubuntu Server
â”‚   â””â”€â”€ models.yaml              # Lista modeli i ich konfiguracje
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-macos.sh           # Setup na MacOS
â”‚   â”œâ”€â”€ setup-ubuntu.sh          # Setup na Ubuntu Server
â”‚   â”œâ”€â”€ start-ollama.sh          # Uruchomienie Ollama
â”‚   â””â”€â”€ deploy.sh                # Deployment na Ubuntu Server
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.ollama        # Container dla Ollama (opcjonalnie)
â”‚   â””â”€â”€ docker-compose.yml       # Orchestracja
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ WL-25-local-support-ollama.md
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                    # Testy jednostkowe
â”‚   â””â”€â”€ integration/             # Testy integracyjne
â””â”€â”€ README.md
```

#### 2. RÃ³Å¼nice miÄ™dzy MacOS (M1) a Ubuntu Server (RTX 3060)

| Aspekt | MacOS M1 (Development) | Ubuntu Server RTX 3060 (Production) |
|--------|------------------------|-------------------------------------|
| **GPU** | Apple Silicon (Metal) | NVIDIA RTX 3060 (CUDA) |
| **WydajnoÅ›Ä‡** | ~10-20 tok/s (CPU/Metal) | ~30-50 tok/s (CUDA) |
| **Modele** | Mniejsze modele (3B-7B) | WiÄ™ksze modele (7B-13B) |
| **Cel** | Development, testy | Production, fallback API |
| **Ollama** | Lokalne testy | Publiczny dostÄ™p przez tunnel |

#### 3. Konfiguracja Å›rodowisk

**Development (MacOS):**
- Ollama lokalnie na `localhost:11434`
- Testy z maÅ‚ymi modelami (Phi-3 Medium 3.8B)
- Brak tunelu (lokalne testy)
- Proste logowanie

**Production (Ubuntu Server):**
- Ollama na `localhost:11434` + reverse proxy
- DuÅ¼e modele (Llama 3.1 8B Q4, Mistral 7B Q4)
- Cloudflare Tunnel lub Tailscale
- PeÅ‚ne bezpieczeÅ„stwo (auth, rate limiting)
- Monitoring i alerty

### ğŸ¯ Faza 1: Development na MacOS (2-3 godziny) âš¡

#### 1.1. Setup podstawowy
- [ ] Utworzenie struktury katalogÃ³w
- [ ] Konfiguracja `.dev/scripts/time.sh` âœ… (zrobione)
- [ ] Utworzenie `README.md` z instrukcjami
- [ ] Uruchomienie Ollama na MacOS: `ollama serve`
- [ ] Test lokalny: `curl http://localhost:11434/api/tags`

#### 1.1a. Migracja Pythona z waldus-api
- [ ] Skopiowanie plikÃ³w z `waldus-api/python/` do `ai-local-core/src/`
  - [ ] `ollama_complete.py` â†’ `src/ollama/complete.py`
  - [ ] `describe_image.py` â†’ `src/image/describe.py`
  - [ ] `translate_text.py` â†’ `src/translation/translate.py`
  - [ ] `api_server.py` â†’ `src/api/server.py`
- [ ] Skopiowanie `requirements.txt` i aktualizacja
- [ ] Skopiowanie dokumentacji (README.md, INSTALLATION_COMPLETE.md, TRANSLATION_SETUP.md)
- [ ] Utworzenie `__init__.py` w kaÅ¼dym module
- [ ] Test kompatybilnoÅ›ci - sprawdzenie czy skrypty dziaÅ‚ajÄ… jako CLI

#### 1.2. Pobranie modelu testowego
```bash
# Na MacOS - maÅ‚y model do testÃ³w
ollama pull phi3:medium
ollama pull llama3.2:3b
```

#### 1.3. Utworzenie OllamaClient (Python) âœ…
- [x] Klasa `OllamaClient` w `src/ollama/client.py` (Python) âœ…
- [x] Metody: `generate()`, `chat()`, `list_models()`, `pull_model()` âœ…
- [x] ObsÅ‚uga bÅ‚Ä™dÃ³w i retry logic âœ…
- [x] UÅ¼ycie biblioteki `requests` do HTTP API âœ…
- [x] KompatybilnoÅ›Ä‡ z istniejÄ…cym `ollama_complete.py` (przeniesionym) âœ…
- [x] Utworzenie `exceptions.py` z wyjÄ…tkami âœ…
- [x] Utworzenie `example.py` z przykÅ‚adem uÅ¼ycia âœ…
- [ ] Refaktoryzacja `complete.py` do uÅ¼ycia `OllamaClient` (opcjonalnie)
- [ ] Testy jednostkowe (pytest)

#### 1.3a. Testy kompatybilnoÅ›ci CLI âœ…
- [x] Test Translation CLI - dziaÅ‚a âœ…
- [x] Test Image Description CLI - dziaÅ‚a âœ…
- [x] Test Ollama CLI - wymaga uruchomionego Ollama âš ï¸
- [x] Test obsÅ‚ugi bÅ‚Ä™dÃ³w (brak argumentÃ³w) - dziaÅ‚a âœ…
- [x] Utworzenie skryptu `scripts/test-cli.sh` âœ…

#### 1.4. Konfiguracja
- [ ] `config/development.yaml` - MacOS settings
- [ ] `config/production.yaml` - Ubuntu Server settings
- [ ] Zmienne Å›rodowiskowe (.env)
- [ ] `requirements.txt` - zaleÅ¼noÅ›ci Python (requests, pydantic, pyyaml)
- [ ] `setup.py` lub `pyproject.toml` - konfiguracja pakietu Python

### ğŸ¯ Faza 2: Testy i walidacja na MacOS (1-2 godziny) ğŸ§ª

#### 2.1. Testy funkcjonalne
- [ ] Test generowania tekstu
- [ ] Test chat completion
- [ ] Test listowania modeli
- [ ] Test obsÅ‚ugi bÅ‚Ä™dÃ³w

#### 2.2. Testy wydajnoÅ›ciowe
- [ ] Pomiar tok/s na MacOS M1
- [ ] PorÃ³wnanie z oczekiwaniami dla RTX 3060
- [ ] Benchmark rÃ³Å¼nych modeli

#### 2.3. Dokumentacja
- [ ] Aktualizacja `README.md`
- [ ] Dokumentacja API
- [ ] PrzykÅ‚ady uÅ¼ycia

### ğŸ¯ Faza 3: Deployment na Ubuntu Server (3-4 godziny) ğŸ–¥ï¸

#### 3.1. Przygotowanie serwera
- [ ] Instalacja Ollama na Ubuntu Server
- [ ] Instalacja NVIDIA drivers + CUDA
- [ ] Pobranie modeli produkcyjnych (Llama 3.1 8B Q4)
- [ ] Test lokalny na serwerze

#### 3.2. BezpieczeÅ„stwo
- [ ] Instalacja Nginx
- [ ] Konfiguracja reverse proxy z Basic Auth
- [ ] Konfiguracja firewall (ufw)
- [ ] Rate limiting

#### 3.3. Tunnel setup
- [ ] WybÃ³r rozwiÄ…zania (Cloudflare Tunnel / Tailscale)
- [ ] Konfiguracja Cloudflare Tunnel lub Tailscale
- [ ] Test dostÄ™pu z zewnÄ…trz
- [ ] Konfiguracja jako systemd service

#### 3.4. Monitoring
- [ ] Logowanie requestÃ³w
- [ ] Monitoring zuÅ¼ycia GPU/VRAM
- [ ] Alerty (opcjonalnie)

### ğŸ¯ Faza 4: Integracja z Waldus API (2-3 godziny) ğŸ”—

#### 4.1. Aktualizacja waldus-api po migracji
- [ ] **Uwaga:** `OllamaProvider` juÅ¼ istnieje w `waldus-api/app/Providers/OllamaProvider.php`
- [ ] Aktualizacja Å›cieÅ¼ek w `OllamaProvider.php`:
  - [ ] Zmiana `base_path('python/ollama_complete.py')` na Å›cieÅ¼kÄ™ do `ai-local-core/src/ollama/complete.py`
  - [ ] MoÅ¼liwoÅ›Ä‡ konfiguracji przez zmiennÄ… Å›rodowiskowÄ… `AI_LOCAL_CORE_PATH`
- [ ] Aktualizacja `ImageDescriptionService.php`:
  - [ ] Zmiana `base_path('python/describe_image.py')` na Å›cieÅ¼kÄ™ do `ai-local-core/src/image/describe.py`
  - [ ] Zmiana `base_path('python/translate_text.py')` na Å›cieÅ¼kÄ™ do `ai-local-core/src/translation/translate.py`
- [ ] Sprawdzenie kompatybilnoÅ›ci z nowym `OllamaClient` (Python)
- [ ] Ewentualna aktualizacja `OllamaProvider.php` do uÅ¼ycia nowego klienta
- [ ] Implementacja zgodna z interfejsem `LLMProvider`
- [ ] ObsÅ‚uga timeoutÃ³w i retry
- [ ] Testy integracyjne - sprawdzenie czy waldus-api dziaÅ‚a z nowymi Å›cieÅ¼kami

#### 4.2. FallbackService integration
- [ ] Dodanie Ollama jako fallback w `FallbackService`
- [ ] Priorytetyzacja: Anthropic â†’ Groq â†’ Ollama
- [ ] Test fallbacku (wyÅ‚Ä…cz Anthropic, sprawdÅº Ollama)

#### 4.3. Konfiguracja
- [ ] Zmienne Å›rodowiskowe dla URL Ollama
- [ ] Konfiguracja timeoutÃ³w
- [ ] Konfiguracja modeli

### ğŸ¯ Faza 5: Optymalizacja i monitoring (opcjonalnie) ğŸš€

- [ ] Optymalizacja wydajnoÅ›ci
- [ ] Monitoring kosztÃ³w energii
- [ ] Automatyczne wyÅ‚Ä…czanie GPU gdy nieuÅ¼ywane
- [ ] Backup konfiguracji

---

## ğŸ“‹ Checklist implementacji

### MacOS (Development)
- [ ] Struktura projektu
- [ ] OllamaClient implementation
- [ ] Testy jednostkowe
- [ ] Dokumentacja
- [ ] Testy lokalne

### Ubuntu Server (Production)
- [ ] Instalacja Ollama + CUDA
- [ ] Modele produkcyjne
- [ ] Nginx reverse proxy
- [ ] Cloudflare Tunnel / Tailscale
- [ ] Monitoring

### Integracja
- [ ] OllamaProvider w Waldus API
- [ ] FallbackService integration
- [ ] Testy end-to-end
- [ ] Dokumentacja deployment

---

## ğŸ”„ Workflow developmentu

1. **Development na MacOS:**
   - Kodowanie i testy lokalne
   - UÅ¼ycie maÅ‚ych modeli do szybkich testÃ³w
   - Commit do repozytorium

2. **Test na Ubuntu Server:**
   - Pull kodu na serwer
   - Uruchomienie skryptÃ³w setup
   - Testy z wiÄ™kszymi modelami

3. **Deployment:**
   - Konfiguracja tunelu
   - Integracja z Waldus API
   - Monitoring

---

## ğŸ’¡ Zalecenia

1. **Rozpocznij od MacOS** - szybki development i testy
2. **UÅ¼yj maÅ‚ych modeli na MacOS** - szybsze iteracje
3. **DuÅ¼e modele tylko na Ubuntu Server** - lepsza wydajnoÅ›Ä‡
4. **Cloudflare Tunnel dla szybkiego startu** - Å‚atwa konfiguracja
5. **Tailscale dla produkcyjnego** - lepsze bezpieczeÅ„stwo

---

## ğŸ Stack technologiczny

### JÄ™zyk programowania: Python

**Uzasadnienie:**
- âœ… W `waldus-api` juÅ¼ istnieje `python/ollama_complete.py` - kompatybilnoÅ›Ä‡
- âœ… `OllamaProvider.php` w Waldus API wywoÅ‚uje Python scripts
- âœ… `ImageDescriptionService.php` uÅ¼ywa `describe_image.py` i `translate_text.py`
- âœ… Biblioteka `ollama` dla Pythona jest dobrze wspierana
- âœ… Åatwa integracja z istniejÄ…cym kodem
- âœ… **Plan:** Przeniesienie caÅ‚ego Pythona z `waldus-api/python/` do `ai-local-core/src/`

**GÅ‚Ã³wne biblioteki:**
- `requests` - HTTP client do Ollama API
- `pydantic` - walidacja danych i modele
- `pyyaml` - konfiguracja YAML
- `pytest` - testy jednostkowe
- `ollama` - oficjalna biblioteka Python dla Ollama
- `torch`, `torchvision` - ML dla rozpoznawania obrazkÃ³w
- `transformers` - modele ML (BLIP dla obrazkÃ³w)
- `pillow` - przetwarzanie obrazÃ³w
- `flask`, `flask-cors` - API server
- `deep-translator` - tÅ‚umaczenie tekstu

**Struktura kodu (po migracji):**
```python
ai-local-core/src/
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py          # OllamaClient class (nowy)
â”‚   â”œâ”€â”€ complete.py       # ollama_complete.py (przeniesiony)
â”‚   â”œâ”€â”€ models.py         # Pydantic models
â”‚   â”œâ”€â”€ exceptions.py     # Custom exceptions
â”‚   â””â”€â”€ utils.py          # Helper functions
â”œâ”€â”€ image/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ describe.py       # describe_image.py (przeniesiony)
â”‚   â””â”€â”€ models.py         # Modele BLIP
â”œâ”€â”€ translation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ translate.py      # translate_text.py (przeniesiony)
â”‚   â””â”€â”€ models.py         # Modele tÅ‚umaczenia
â””â”€â”€ api/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ server.py         # api_server.py (przeniesiony)
```

**KompatybilnoÅ›Ä‡ z waldus-api:**
- Nowy `OllamaClient` bÄ™dzie kompatybilny z istniejÄ…cym `ollama_complete.py`
- Wszystkie skrypty bÄ™dÄ… dostÄ™pne jako CLI (zachowanie kompatybilnoÅ›ci)
- MoÅ¼liwoÅ›Ä‡ uÅ¼ycia jako zamiennik lub rozszerzenie
- Zachowanie tego samego interfejsu API

---

## ğŸ“¦ Plan migracji Pythona z waldus-api

### Cel migracji

Przeniesienie caÅ‚ego kodu Pythona z `waldus-api/python/` do `ai-local-core/src/` w celu:
- âœ… Centralizacji wszystkich lokalnych usÅ‚ug ML/LLM w jednym miejscu
- âœ… Lepszej organizacji kodu
- âœ… Åatwiejszego zarzÄ…dzania zaleÅ¼noÅ›ciami
- âœ… MoÅ¼liwoÅ›ci rozwoju ML/rozpoznawania obrazkÃ³w w dedykowanym projekcie

### Pliki do przeniesienia

| Plik ÅºrÃ³dÅ‚owy (waldus-api) | Plik docelowy (ai-local-core) | Opis |
|----------------------------|-------------------------------|------|
| `python/ollama_complete.py` | `src/ollama/complete.py` | Komunikacja z Ollama API |
| `python/describe_image.py` | `src/image/describe.py` | Rozpoznawanie obrazkÃ³w (BLIP) |
| `python/translate_text.py` | `src/translation/translate.py` | TÅ‚umaczenie tekstu |
| `python/api_server.py` | `src/api/server.py` | Flask API server |
| `python/requirements.txt` | `requirements.txt` | ZaleÅ¼noÅ›ci Python |
| `python/README.md` | `docs/python-migration.md` | Dokumentacja (merge) |
| `python/INSTALLATION_COMPLETE.md` | `docs/installation.md` | Instrukcje instalacji |
| `python/TRANSLATION_SETUP.md` | `docs/translation-setup.md` | Setup tÅ‚umaczenia |
| `python/run_api.sh` | `scripts/run-api.sh` | Skrypt uruchomienia API |

### Aktualizacje w waldus-api

Po migracji naleÅ¼y zaktualizowaÄ‡ Å›cieÅ¼ki w nastÄ™pujÄ…cych plikach:

#### 1. `app/Providers/OllamaProvider.php`
```php
// PRZED:
$scriptPath = base_path('python/ollama_complete.py');

// PO:
$aiLocalCorePath = env('AI_LOCAL_CORE_PATH', '/path/to/ai-local-core');
$scriptPath = $aiLocalCorePath . '/src/ollama/complete.py';
```

#### 2. `app/Services/ImageDescriptionService.php`
```php
// PRZED:
$this->pythonScript = base_path('python/describe_image.py');
$translateScript = base_path('python/translate_text.py');

// PO:
$aiLocalCorePath = env('AI_LOCAL_CORE_PATH', '/path/to/ai-local-core');
$this->pythonScript = $aiLocalCorePath . '/src/image/describe.py';
$translateScript = $aiLocalCorePath . '/src/translation/translate.py';
```

#### 3. Konfiguracja `.env` w waldus-api
```env
# ÅšcieÅ¼ka do projektu ai-local-core
AI_LOCAL_CORE_PATH=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core

# Alternatywnie: Å›cieÅ¼ka bezwzglÄ™dna do skryptÃ³w
OLLAMA_SCRIPT_PATH=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core/src/ollama/complete.py
IMAGE_DESCRIPTION_SCRIPT_PATH=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core/src/image/describe.py
TRANSLATION_SCRIPT_PATH=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core/src/translation/translate.py
```

### Zachowanie kompatybilnoÅ›ci

**WaÅ¼ne:** Wszystkie przeniesione skrypty muszÄ… zachowaÄ‡ kompatybilnoÅ›Ä‡ CLI:

1. **Ollama complete:**
   ```bash
   # DziaÅ‚a tak samo jak wczeÅ›niej
   python src/ollama/complete.py '{"user": "Hello", "temperature": 0.7}'
   ```

2. **Image description:**
   ```bash
   # DziaÅ‚a tak samo jak wczeÅ›niej
   python src/image/describe.py "https://example.com/image.jpg" 50
   ```

3. **Translation:**
   ```bash
   # DziaÅ‚a tak samo jak wczeÅ›niej
   python src/translation/translate.py "Hello world" pl
   ```

### KolejnoÅ›Ä‡ migracji

1. **Faza 1:** Skopiowanie plikÃ³w do `ai-local-core/src/`
2. **Faza 2:** Testy kompatybilnoÅ›ci CLI
3. **Faza 3:** Aktualizacja Å›cieÅ¼ek w `waldus-api`
4. **Faza 4:** Testy integracyjne z `waldus-api`
5. **Faza 5:** UsuniÄ™cie starych plikÃ³w z `waldus-api/python/` (opcjonalnie)

### KorzyÅ›ci z migracji

- âœ… **Centralizacja:** Wszystkie lokalne usÅ‚ugi ML/LLM w jednym miejscu
- âœ… **RozwÃ³j:** Åatwiejsze dodawanie nowych funkcji ML
- âœ… **ZarzÄ…dzanie:** Jeden `requirements.txt`, jedna struktura projektu
- âœ… **Testy:** Åatwiejsze testowanie caÅ‚ego stacku ML
- âœ… **Deployment:** MoÅ¼liwoÅ›Ä‡ deployowania jako osobny serwis

---

## ğŸ”— Å¹rÃ³dÅ‚a

- [Ollama Installation](https://ollama.ai/download)
- [Cloudflare Tunnel Docs](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/)
- [Tailscale Docs](https://tailscale.com/kb/)
- [Ollama Models](https://ollama.ai/library)
- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)

---

**Autor analizy:** Auto (Agent Router by Cursor)  
**Data:** 2025-11-09  
**Status:** ğŸŸ¡ W toku

