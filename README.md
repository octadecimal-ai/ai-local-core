# ai-local-core - Lokalne usÅ‚ugi ML/LLM dla Waldus API

Projekt zawiera lokalne implementacje usÅ‚ug Machine Learning i Large Language Models, ktÃ³re mogÄ… byÄ‡ uÅ¼ywane jako fallback dla zewnÄ™trznych API.

## ğŸ“‹ ZawartoÅ›Ä‡ projektu

- **Ollama Client** - Komunikacja z lokalnym serwerem Ollama (LLM)
- **Image Description** - Rozpoznawanie i opisywanie obrazkÃ³w (BLIP model)
- **Translation** - TÅ‚umaczenie tekstu
- **API Server** - Flask API server dla wszystkich usÅ‚ug

## ğŸš€ Szybki start

### Instalacja

1. **UtwÃ³rz virtual environment (zalecane):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Linux/Mac
   # lub
   venv\Scripts\activate  # Windows
   ```

2. **Zainstaluj zaleÅ¼noÅ›ci Python:**
   ```bash
   pip install --upgrade pip setuptools wheel
   pip install -r requirements.txt
   ```

   **Uwaga:** Instalacja moÅ¼e zajÄ…Ä‡ kilka minut, szczegÃ³lnie `torch` i `transformers` sÄ… duÅ¼e.

3. **Aktywuj Å›rodowisko (skrypt pomocniczy):**
   ```bash
   source scripts/activate.sh
   ```

2. **Zainstaluj Ollama** (jeÅ›li jeszcze nie masz):
   - MacOS: `brew install ollama` lub pobierz z [ollama.ai](https://ollama.ai/download)
   - Linux: `curl -fsSL https://ollama.ai/install.sh | sh`

3. **Pobierz modele Ollama:**
   ```bash
   ollama pull llama3.1:8b
   ollama pull phi3:medium
   ```

## ğŸ“ Struktura projektu

```
ai-local-core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ollama/          # Komunikacja z Ollama API
â”‚   â”œâ”€â”€ image/           # Rozpoznawanie obrazkÃ³w
â”‚   â”œâ”€â”€ translation/     # TÅ‚umaczenie tekstu
â”‚   â””â”€â”€ api/             # Flask API server
â”œâ”€â”€ config/              # Konfiguracje (development/production)
â”œâ”€â”€ scripts/             # Skrypty pomocnicze
â”œâ”€â”€ docs/                # Dokumentacja
â””â”€â”€ requirements.txt     # ZaleÅ¼noÅ›ci Python
```

## ğŸ”§ UÅ¼ycie

### Ollama Client

#### CLI (kompatybilnoÅ›Ä‡ z waldus-api)

```bash
# Podstawowe uÅ¼ycie
python src/ollama/complete.py '{"user": "Hello, how are you?", "temperature": 0.7}'

# Z system prompt
python src/ollama/complete.py '{"system": "You are a helpful assistant", "user": "What is Python?", "max_tokens": 100}'
```

#### Python API (nowy OllamaClient)

```python
from ollama.client import OllamaClient

# UtwÃ³rz klienta
client = OllamaClient()

# SprawdÅº dostÄ™pnoÅ›Ä‡
if client.check_health():
    # Chat completion
    result = client.chat(
        user="Hello, how are you?",
        system="You are a helpful assistant",
        temperature=0.7,
        max_tokens=100
    )
    print(result['text'])
    
    # Lista modeli
    models = client.list_models()
    for model in models:
        print(model['name'])
    
    # Generate
    result = client.generate(
        prompt="Write a short poem about programming",
        temperature=0.8
    )
    print(result['text'])
```

#### PrzykÅ‚ad uÅ¼ycia

```bash
python src/ollama/example.py
```

#### Prosty skrypt do zadawania pytaÅ„

Edytuj zmienne w `scripts/ask_ollama.py` i uruchom:

```bash
# Edytuj PYTANIE, SYSTEM_PROMPT, MODEL, etc. w scripts/ask_ollama.py
python scripts/ask_ollama.py
```

PrzykÅ‚ad konfiguracji w `scripts/ask_ollama.py`:
```python
PYTANIE = "Co to jest Python? Odpowiedz krÃ³tko."
SYSTEM_PROMPT = "JesteÅ› pomocnym asystentem. Odpowiadaj po polsku."
MODEL = None  # None = uÅ¼yj domyÅ›lnego
TEMPERATURE = 0.7
MAX_TOKENS = 200
```

### Image Description (CLI)

```bash
# Opisz obraz z URL
python src/image/describe.py "https://example.com/image.jpg" 50

# Opisz obraz z lokalnego pliku
python src/image/describe.py "/path/to/image.jpg" 50
```

### Translation (CLI)

```bash
# TÅ‚umacz na polski
python src/translation/translate.py "Hello world" pl

# TÅ‚umacz na niemiecki
python src/translation/translate.py "Hello world" de
```

### API Server

```bash
# Uruchom Flask API server
python src/api/server.py

# Server bÄ™dzie dostÄ™pny na http://127.0.0.1:5001
```

## ğŸ”— Integracja z Waldus API

Po migracji, w `waldus-api` naleÅ¼y zaktualizowaÄ‡ Å›cieÅ¼ki:

1. **Dodaj do `.env`:**
   ```env
   AI_LOCAL_CORE_PATH=/Users/piotradamczyk/Projects/Octadecimal/ai-local-core
   ```

2. **Aktualizuj `OllamaProvider.php`:**
   ```php
   $aiLocalCorePath = env('AI_LOCAL_CORE_PATH');
   $scriptPath = $aiLocalCorePath . '/src/ollama/complete.py';
   ```

3. **Aktualizuj `ImageDescriptionService.php`:**
   ```php
   $aiLocalCorePath = env('AI_LOCAL_CORE_PATH');
   $this->pythonScript = $aiLocalCorePath . '/src/image/describe.py';
   $translateScript = $aiLocalCorePath . '/src/translation/translate.py';
   ```

## ğŸ“š Dokumentacja

- [Plan dziaÅ‚ania i architektura](docs/WL-25-local-support-ollama.md)
- [Instalacja i setup](docs/installation.md)
- [Setup tÅ‚umaczenia](docs/translation-setup.md)
- [Plan migracji Pythona](docs/python-migration.md)
- [Testy jednostkowe - peÅ‚na dokumentacja](docs/testing.md)
- [Konfiguracja z serwerem OVH](docs/ovh-server-setup.md) â­
- [Konfiguracja tunelu (Cloudflare/Tailscale)](docs/tunnel-setup.md)
- [Rekomendacje modeli Ollama](docs/model-recommendations.md)

## ğŸ§ª Testy jednostkowe

### Instalacja pytest (jeÅ›li jeszcze nie zainstalowane)

```bash
source venv/bin/activate
pip install pytest pytest-cov
```

### Uruchamianie testÃ³w

#### Podstawowe komendy

```bash
# Aktywuj virtual environment
source venv/bin/activate

# Wszystkie testy jednostkowe
pytest tests/unit/ -v

# Wszystkie testy (unit + integration)
pytest tests/ -v

# Tylko testy OllamaClient
pytest tests/unit/test_ollama_client.py -v

# Tylko testy Translation
pytest tests/unit/test_translation.py -v
```

#### Z raportem pokrycia kodem (coverage)

```bash
# Coverage w terminalu
pytest tests/ --cov=src --cov-report=term-missing

# Coverage z raportem HTML (otwÃ³rz htmlcov/index.html)
pytest tests/ --cov=src --cov-report=html --cov-report=term-missing

# Tylko testy jednostkowe z coverage
pytest tests/unit/ --cov=src --cov-report=term-missing
```

#### UÅ¼ycie skryptu pomocniczego

```bash
# Uruchom wszystkie testy z coverage
./scripts/run-tests.sh

# Lub z dodatkowymi opcjami pytest
./scripts/run-tests.sh -v --tb=short
```

### PrzykÅ‚adowe wyniki

```bash
$ pytest tests/unit/ -v

============================= test session starts ==============================
platform darwin -- Python 3.12.8, pytest-9.0.0
collected 18 items

tests/unit/test_ollama_client.py::TestOllamaClient::test_init_default PASSED
tests/unit/test_ollama_client.py::TestOllamaClient::test_chat_success PASSED
...
tests/unit/test_translation.py::TestTranslation::test_translate_text_pl PASSED

============================== 18 passed in 3.48s ==============================
```

### Status testÃ³w

- âœ… **OllamaClient** - 15 testÃ³w, wszystkie przechodzÄ… (96% coverage)
- âœ… **Translation** - 3 testy, wszystkie przechodzÄ…
- â³ Image description - do dodania
- â³ API server - do dodania

### Struktura testÃ³w

```
tests/
â”œâ”€â”€ unit/                    # Testy jednostkowe
â”‚   â”œâ”€â”€ test_ollama_client.py    # 15 testÃ³w
â”‚   â””â”€â”€ test_translation.py      # 3 testy
â””â”€â”€ integration/             # Testy integracyjne (do dodania)
```

### Opcje pytest

```bash
# Verbose output (szczegÃ³Å‚owe)
pytest tests/ -v

# Bardzo szczegÃ³Å‚owe (pokazuje printy)
pytest tests/ -v -s

# KrÃ³tki traceback przy bÅ‚Ä™dach
pytest tests/ --tb=short

# Tylko pierwszy bÅ‚Ä…d
pytest tests/ -x

# Uruchom konkretny test
pytest tests/unit/test_ollama_client.py::TestOllamaClient::test_chat_success -v
```

### Troubleshooting

**Problem: ModuleNotFoundError**
```bash
# Upewnij siÄ™, Å¼e jesteÅ› w katalogu projektu
cd /Users/piotradamczyk/Projects/Octadecimal/ai-local-core

# Aktywuj virtual environment
source venv/bin/activate

# SprawdÅº czy pytest jest zainstalowany
pip list | grep pytest
```

**Problem: Import errors**
```bash
# Upewnij siÄ™, Å¼e Å›cieÅ¼ka do src jest poprawna
# Testy automatycznie dodajÄ… src do PYTHONPATH
```

### Test importÃ³w
```bash
# UÅ¼yj skryptu pomocniczego
./scripts/test-imports.sh

# Lub rÄ™cznie
source venv/bin/activate
python3 -c "import sys; sys.path.insert(0, 'src'); from ollama.complete import complete; print('âœ… OK')"
```

### Test funkcjonalnoÅ›ci
```bash
source venv/bin/activate

# Test Ollama (wymaga uruchomionego Ollama)
python src/ollama/complete.py '{"user": "Test"}'

# Test Image Description (wymaga modelu BLIP - zaÅ‚aduje siÄ™ automatycznie)
python src/image/describe.py "https://picsum.photos/800/600" 50

# Test Translation
python src/translation/translate.py "Hello world" pl
```

## ğŸ”„ RozwiÄ…zanie Polling (Rekomendowane) â­â­â­

Najprostsze rozwiÄ…zanie - lokalny serwer pyta serwer OVH czy ma zapytanie:

```bash
# Uruchom klienta polling
./scripts/start-polling-client.sh

# Z wÅ‚asnymi parametrami
POLLING_SERVER_URL="https://waldus-server.com" \
POLLING_INTERVAL=5 \
./scripts/start-polling-client.sh
```

**Jak to dziaÅ‚a:**
1. Lokalny PC co kilka sekund pyta serwer OVH: "Masz coÅ› dla mnie?"
2. JeÅ›li serwer ma zapytanie - lokalny PC przetwarza przez Ollama
3. Lokalny PC zwraca odpowiedÅº na serwer OVH
4. Serwer OVH zwraca odpowiedÅº do Waldus API

**Zalety:**
- âœ… Proste - nie wymaga VPN, tuneli, kluczy
- âœ… Niezawodne - dziaÅ‚a przez standardowe HTTP
- âœ… DziaÅ‚a z NAT - nie wymaga publicznego IP

**Dokumentacja:** [RozwiÄ…zanie Polling](docs/polling-solution.md)

## ğŸŒ Konfiguracja tunelu (dostÄ™p z zewnÄ…trz)

Aby umoÅ¼liwiÄ‡ dostÄ™p do Ollama z serwera Waldus API, musisz skonfigurowaÄ‡ tunel.

### Opcja 1: WÅ‚asny serwer OVH â­â­â­ (Rekomendowane)

JeÅ›li masz serwer na OVH, moÅ¼esz skonfigurowaÄ‡ wÅ‚asny tunel:

**SSH Tunnel (najprostsze):**
```bash
./scripts/setup-ssh-tunnel.sh
```

**WireGuard VPN (najlepsze):**
```bash
./scripts/setup-wireguard-ovh.sh
```

**Nginx Reverse Proxy z SSL:**
```bash
# Na serwerze OVH
./scripts/setup-nginx-ovh.sh
```

**Dokumentacja:** [Konfiguracja z serwerem OVH](docs/ovh-server-setup.md)

### Opcja 2: Cloudflare Tunnel (szybki start)

```bash
# Instalacja (MacOS)
brew install cloudflared

# Lub uÅ¼yj skryptu
./scripts/setup-cloudflare-tunnel.sh

# Uruchomienie tunelu
./scripts/start-tunnel.sh
```

**Wynik:** Otrzymasz URL (np. `https://xxx.trycloudflare.com`), ktÃ³ry moÅ¼esz uÅ¼yÄ‡ w konfiguracji Waldus API.

### Opcja 3: Tailscale VPN

```bash
# Instalacja (MacOS)
brew install tailscale

# Lub uÅ¼yj skryptu
./scripts/setup-tailscale.sh

# PoÅ‚Ä…czenie
tailscale up
```

**Wynik:** Otrzymasz Tailscale IP (np. `100.x.x.x`), ktÃ³re moÅ¼esz uÅ¼yÄ‡ w konfiguracji.

### Dokumentacja

- [Konfiguracja z serwerem OVH](docs/ovh-server-setup.md) â­
- [Konfiguracja tunelu (Cloudflare/Tailscale)](docs/tunnel-setup.md)

## ğŸ”’ BezpieczeÅ„stwo

- Wszystkie skrypty sÄ… przeznaczone do lokalnego uÅ¼ycia
- Dla produkcyjnego uÅ¼ycia rozwaÅ¼:
  - Reverse proxy z autoryzacjÄ… (Nginx) - zobacz [dokumentacjÄ™ tunelu](docs/tunnel-setup.md#-bezpieczeÅ„stwo)
  - Cloudflare Tunnel lub Tailscale dla bezpiecznego dostÄ™pu
  - Rate limiting

## ğŸ“ Status

- âœ… Migracja Pythona z waldus-api
- âœ… Struktura projektu
- â³ OllamaClient implementation
- â³ Testy jednostkowe
- â³ Integracja z Waldus API

## ğŸ¤ Wsparcie

W razie problemÃ³w sprawdÅº:
- [DokumentacjÄ™ Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [DokumentacjÄ™ BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base)
- Logi w `.dev/logs/cursor/`

---

**Autor:** Auto (Agent Router by Cursor)  
**Data utworzenia:** 2025-11-09  
**Status:** ğŸŸ¡ W rozwoju

